@InProceedings{Caron2021,
  author    = {Mathilde Caron and Hugo Touvron and Ishan Misra and Herve Jegou and Julien Mairal and Piotr Bojanowski and Armand Joulin},
  booktitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
  date      = {2021},
  title     = {Emerging Properties in Self-Supervised Vision Transformers},
  doi       = {10.1109/iccv48922.2021.00951},
  publisher = {{IEEE}},
  abstract  = {1. Introduction},
  file      = {:Unsupervised Segmentation, Transformer/Caron2021 - Emerging Properties in Self Supervised Vision Transformers.pdf:PDF},
  groups    = {Unsupervised Segmentation, Transformer},
  year      = {2021},
}

@Article{Baltruschat2021,
  author           = {Ivo M. Baltruschat and Hanna {\'{C}}wieka and Diana Krüger and Berit Zeller-Plumhoff and Frank Schlünzen and Regine Willumeit-Römer and Julian Moosmann and Philipp Heuser},
  date             = {2021},
  journaltitle     = {Scientific Reports},
  title            = {Scaling the U-net: Segmentation of biodegradable bone implants in high-resolution synchrotron radiation microtomograms},
  doi              = {10.1038/s41598-021-03542-y},
  number           = {1},
  volume           = {11},
  file             = {:CNN/Baltruschat2021 - Scaling the U Net_ Segmentation of Biodegradable Bone Implants in High Resolution Synchrotron Radiation Microtomograms.pdf:PDF},
  groups           = {CNN},
  modificationdate = {2022-03-02T11:57:24},
  publisher        = {Springer Science and Business Media {LLC}},
  year             = {2021},
}

@InCollection{Isensee2019,
  author           = {Fabian Isensee and Jens Petersen and Andre Klein and David Zimmerer and Paul F. Jaeger and Simon Kohl and Jakob Wasserthal and Gregor Koehler and Tobias Norajitra and Sebastian Wirkert and Klaus H. Maier-Hein},
  booktitle        = {Informatik aktuell},
  date             = {2019},
  title            = {{nnU}-Net: Self-adapting Framework for U-Net-Based Medical Image Segmentation},
  doi              = {10.1007/978-3-658-25326-4_7},
  publisher        = {Springer Fachmedien Wiesbaden},
  comment          = {\url{https://www.nature.com/articles/s41592-020-01008-z#additional-information}},
  creationdate     = {2022-03-01T17:39:47},
  file             = {:CNN/Isensee2019 - NnU Net_ Self Adapting Framework for U Net Based Medical Image Segmentation.pdf:PDF},
  groups           = {CNN},
  issn             = {1431-472X},
  keywords         = {Semantic Segmentation, Medical Imaging, U-Net},
  modificationdate = {2022-03-02T14:00:17},
  priority         = {prio3},
  year             = {2019},
}

@InCollection{Ronneberger2015,
  author           = {Olaf Ronneberger and Philipp Fischer and Thomas Brox},
  booktitle        = {Lecture Notes in Computer Science},
  title            = {U-Net: Convolutional Networks for Biomedical Image Segmentation},
  doi              = {10.1007/978-3-319-24574-4_28},
  pages            = {234--241},
  publisher        = {Springer International Publishing},
  creationdate     = {2022-03-01T17:48:27},
  file             = {:CNN/Ronneberger2015 - U Net_ Convolutional Networks for Biomedical Image Segmentation.pdf:PDF},
  groups           = {CNN},
  issn             = {0302-9743},
  modificationdate = {2022-03-02T11:57:17},
  year             = {2015},
}

@Misc{Jing2019,
  author    = {Jing, Longlong and Tian, Yingli},
  title     = {Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey},
  doi       = {10.48550/ARXIV.1902.06162},
  abstract  = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual
feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating
large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general
image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive
review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation,
general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for
self-supervised learning are summarized. Next, the schema and evaluation metrics of self-supervised learning methods are reviewed
followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally,
quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image
and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual
feature learning.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file      = {:DL/Jing2019 - Self Supervised Visual Feature Learning with Deep Neural Networks_ a Survey.pdf:PDF},
  groups    = {DL},
  issn      = {0162-8828},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  pages     = {4037--4058},
  publisher = {arXiv},
  volume    = {43},
  year      = {2019},
}

@Article{Shurrab2021,
  author       = {Saaed Shurrab and Rehab Duwairi},
  journaltitle = {{PeerJ} Computer Science},
  title        = {Self-supervised learning methods and applications in medical imaging analysis: A survey},
  doi          = {10.48550/arXiv.2109.08685},
  comment      = {Einteilung nach pretext-Task (Shurrab2021): predictive, generative and contrastive
◦ predictive
    ▪ Exemplar CNN
    ▪ Relative position prediction
    ▪ Jigsaw puzzle
    ▪ rotation prediction
◦ generative
    ▪ denoising auto-encoders
    ▪ image inpainting
    ▪ image colorization
    ▪ split-brain auto-encoder
    ▪ Deep Convolutionsl GAN
    ▪ bi-directional GAN
◦ Cotrasive
    ▪ CPC
    ▪ MoCo
    ▪ SimCLR
    ▪ BYOL
    ▪ SwAV},
  copyright    = {Creative Commons Attribution Non Commercial No Derivatives 4.0 International},
  file         = {:Semi-Supervised Segmentation/Shurrab2021 - Self Supervised Learning Methods and Applications in Medical Imaging Analysis_ a Survey.pdf:PDF},
  groups       = {Semi-Supervised Segmentation},
  keywords     = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences},
  priority     = {prio1},
  publisher    = {arXiv},
  year         = {2021},
}

@PhdThesis{Baltruschat2021a,
  author = {Ivo Baltruschat},
  title  = {Deep Learning for Automatic LungDisease Analysis in Chest X-rays},
  file   = {:CNN/Baltruschat2021a - Deep Learning for Automatic LungDisease Analysis in Chest X Rays.pdf:PDF},
  groups = {CNN},
  school = {Technische Universität Hamburg},
  year   = {2021},
}

@Book{Burkov2019,
  author    = {Burkov, Andriy},
  date      = {2019},
  title     = {The Hundred-Page Machine Learning Book},
  isbn      = {1999579518},
  publisher = {Lightning Source inc.},
  comment   = {Preface
 1 Introduction
    1.1 What is Machine Learning
    1.2 Types of Learning
        1.2.1 Supervised Learning
        1.2.2 Unsupervised Learning
        1.2.3 Semi-Supervised Learning
        1.2.4 Reinforcement Learning
    1.3 How Supervised Learning Works
    1.4 Why the Model Works on New Data
 2 Notation and Definitions
    2.1 Notation
        2.1.1 Data Structures
        2.1.2 Capital Sigma Notation
        2.1.3 Capital Pi Notation
        2.1.4 Operations on Sets
        2.1.5 Operations on Vectors
        2.1.6 Functions
        2.1.7 Max and Arg Max
        2.1.8 Assignment Operator
        2.1.9 Derivative and Gradient
    2.2 Random Variable
    2.3 Unbiased Estimators
    2.4 Bayes’ Rule
    2.5 Parameter Estimation
    2.6 Parameters vs. Hyperparameters
    2.7 Classification vs. Regression
    2.8 Model-Based vs. Instance-Based Learning
    2.9 Shallow vs. Deep Learning
 3 Fundamental Algorithms
    3.1 Linear Regression
        3.1.1 Problem Statement
        3.1.2 Solution
    3.2 Logistic Regression
        3.2.1 Problem Statement
        3.2.2 Solution
    3.3 Decision Tree Learning
        3.3.1 Problem Statement
        3.3.2 Solution
    3.4 Support Vector Machine
        3.4.1 Dealing with Noise
        3.4.2 Dealing with Inherent Non-Linearity
    3.5 k-Nearest Neighbors
 4 Anatomy of a Learning Algorithm
    4.1 Building Blocks of a Learning Algorithm
    4.2 Gradient Descent
    4.3 How Machine Learning Engineers Work
    4.4 Learning Algorithms’ Particularities
 5 Basic Practice
    5.1 Feature Engineering
        5.1.1 One-Hot Encoding
        5.1.2 Binning
        5.1.3 Normalization
        5.1.4 Standardization
        5.1.5 Dealing with Missing Features
        5.1.6 Data Imputation Techniques
    5.2 Learning Algorithm Selection
    5.3 Three Sets
    5.4 Underfitting and Overfitting
    5.5 Regularization
    5.6 Model Performance Assessment
        5.6.1 Confusion Matrix
        5.6.2 Precision/Recall
        5.6.3 Accuracy
        5.6.4 Cost-Sensitive Accuracy
        5.6.5 Area under the ROC Curve (AUC)
    5.7 Hyperparameter Tuning
        5.7.1 Cross-Validation
 6 Neural Networks and Deep Learning
    6.1 Neural Networks
        6.1.1 Multilayer Perceptron Example
        6.1.2 Feed-Forward Neural Network Architecture
    6.2 Deep Learning
        6.2.1 Convolutional Neural Network
        6.2.2 Recurrent Neural Network
 7 Problems and Solutions
    7.1 Kernel Regression
    7.2 Multiclass Classification
    7.3 One-Class Classification
    7.4 Multi-Label Classification
    7.5 Ensemble Learning
        7.5.1 Boosting and Bagging
        7.5.2 Random Forest
        7.5.3 Gradient Boosting
    7.6 Learning to Label Sequences
    7.7 Sequence-to-Sequence Learning
    7.8 Active Learning
    7.9 Semi-Supervised Learning
    7.10 One-Shot Learning
    7.11 Zero-Shot Learning
 8 Advanced Practice
    8.1 Handling Imbalanced Datasets
    8.2 Combining Models
    8.3 Training Neural Networks
    8.4 Advanced Regularization
    8.5 Handling Multiple Inputs
    8.6 Handling Multiple Outputs
    8.7 Transfer Learning
    8.8 Algorithmic Efficiency
 9 Unsupervised Learning
    9.1 Density Estimation
    9.2 Clustering
        9.2.1 K-Means
        9.2.2 DBSCAN and HDBSCAN
        9.2.3 Determining the Number of Clusters
        9.2.4 Other Clustering Algorithms
    9.3 Dimensionality Reduction
        9.3.1 Principal Component Analysis
        9.3.2 UMAP
    9.4 Outlier Detection
 10 Other Forms of Learning
    10.1 Metric Learning
    10.2 Learning to Rank
    10.3 Learning to Recommend
        10.3.1 Factorization Machines
        10.3.2 Denoising Autoencoders
    10.4 Self-Supervised Learning: Word Embeddings
 11 Conclusion
    11.1 Topic Modeling
    11.2 Gaussian Processes
    11.3 Generalized Linear Models
    11.4 Probabilistic Graphical Models
    11.5 Markov Chain Monte Carlo
    11.6 Genetic Algorithms
    11.7 Reinforcement Learning Index},
  ean       = {9781999579517},
  file      = {:ML&AI/Burkov2019 - The Hundred Page Machine Learning Book.pdf:PDF},
  groups    = {ML&AI},
  year      = {2019},
}

@Article{Seo2020,
  author       = {Hyunseok Seo and Masoud Badiei Khuzani and Varun Vasudevan and Charles Huang and Hongyi Ren and Ruoxiu Xiao and Xiao Jia and Lei Xing},
  journaltitle = {Medical Physics},
  title        = {Machine learning techniques for biomedical image segmentation: An overview of technical aspects and introduction to state-of-art applications},
  doi          = {10.1002/mp.13649},
  number       = {5},
  volume       = {47},
  file         = {:Segmentation/Seo2020 - Machine Learning Techniques for Biomedical Image Segmentation_ an Overview of Technical Aspects and Introduction to State of Art Applications.pdf:PDF},
  groups       = {Segmentation},
  journal      = {Medical Physics},
  month        = may,
  publisher    = {Wiley},
  year         = {2020},
}

@WWW{Bandyopadhyay2022,
  author = {Hmrishav Bandyopadhyay},
  date   = {2022-03-15},
  editor = {{V7Labs}},
  title  = {A Gentle Introduction to Image Segmentation for Machine Learning},
  url    = {https://www.v7labs.com/blog/image-segmentation-guide},
  note   = {commercial!},
  file   = {:Segmentation/Bandyopadhyay2022 - A Gentle Introduction to Image Segmentation for Machine Learning.pdf:PDF},
  groups = {Segmentation},
  year   = {2022},
}

@Article{Vrana2021,
  author       = {Johannes Vrana and Ripudaman Singh},
  date         = {2021-01-03},
  journaltitle = {Journal of Nondestructive Evaluation},
  title        = {{NDE} 4.0{\textemdash}A Design Thinking Perspective},
  doi          = {10.1007/s10921-020-00735-9},
  number       = {1},
  volume       = {40},
  file         = {:ML&AI/Vrana2021 - NDE 4.0_A Design Thinking Perspective.pdf:PDF},
  groups       = {ML&AI},
  keywords     = {NDE 4.0, Use cases, Value proposition, Design thinking, Future of NDE, NDT 4.0},
  publisher    = {Springer Science and Business Media {LLC}},
  year         = {2021},
}

@Article{LeCun2015,
  author       = {Yann LeCun and Yoshua Bengio and Geoffrey Hinton},
  date         = {2015-05},
  journaltitle = {Nature},
  title        = {Deep learning},
  doi          = {10.1038/nature14539},
  number       = {7553},
  pages        = {436--444},
  volume       = {521},
  file         = {:DL/LeCun2015 - Deep Learning.pdf:PDF},
  groups       = {DL},
  publisher    = {Springer Science and Business Media {LLC}},
}

@Article{EmmertStreib2020,
  author       = {Frank Emmert-Streib and Zhen Yang and Han Feng and Shailesh Tripathi and Matthias Dehmer},
  date         = {2020},
  journaltitle = {Frontiers in Artificial Intelligence},
  title        = {An Introductory Review of Deep Learning for Prediction Models With Big Data},
  doi          = {10.3389/frai.2020.00004},
  volume       = {3},
  abstract     = {Deep learning models stand for a new learning paradigm in artificial intelligence (AI) and machine learning.},
  comment      = {do I need this?},
  file         = {:DL/EmmertStreib2020 - An Introductory Review of Deep Learning for Prediction Models with Big Data.pdf:PDF},
  groups       = {DL},
  keywords     = {deep learning, artificial intelligence, machine learning, neural networks, prediction models, data science},
  publisher    = {Frontiers Media {SA}},
}

@Book{Rich1983,
  author    = {Elaine Rich},
  date      = {1983},
  title     = {Artificial Intelligence},
  publisher = {McGraw-Hill},
  file      = {:ML&AI/Rich1983 - Artificial Intelligence.pdf:PDF},
  groups    = {ML&AI},
}

@Book{Mitchell1997,
  author    = {Mitchell, Tom},
  date      = {1997},
  title     = {Machine Learning},
  isbn      = {0070428077},
  location  = {New York},
  publisher = {McGraw-Hill},
  abstract  = {Machine Learning},
  file      = {:ML&AI/Mitchell1997 - Machine Learning.pdf:PDF},
  groups    = {ML&AI},
}

@Article{Swana2021,
  author       = {Fezeka Swana and Wesley Doorsamy},
  date         = {2021},
  journaltitle = {Energies},
  title        = {An Unsupervised Learning Approach to Condition Assessment on a Wound-Rotor Induction Generator},
  doi          = {10.3390/en14030602},
  issn         = {1996-1073},
  number       = {3},
  pages        = {602},
  volume       = {14},
  abstract     = {Accurate online diagnosis of incipient faults and condition assessment on generators is especially challenging to automate through supervised learning techniques, because of data imbalance. Fault-condition training and test data are either not available or are experimentally emulated, and therefore do not precisely account for all the eventualities and nuances of practical operating conditions. Thus, it would be more convenient to harness the ability of unsupervised learning in these applications. An investigation into the use of unsupervised learning as a means of recognizing incipient fault patterns and assessing the condition of a wound-rotor induction generator is presented. High-dimension clustering is performed using stator and rotor current and voltage signatures measured under healthy and varying fault conditions on an experimental wound-rotor induction generator. An analysis and validation of the clustering results are carried out to determine the performance and suitability of the technique. Results indicate that the presented technique can accurately distinguish the different incipient faults investigated in an unsupervised manner. This research will contribute to the ongoing development of unsupervised learning frameworks in data-driven diagnostic systems for WRIGs and similar electrical machines.},
  file         = {:ML&AI/Swana2021 - An Unsupervised Learning Approach to Condition Assessment on a Wound Rotor Induction Generator.pdf:PDF},
  groups       = {ML&AI},
  keywords     = {unsupervised learning; wound-rotor induction generator; incipient fault; condition assessment; predictive maintenance},
  publisher    = {{MDPI} {AG}},
}

@Book{Skansi2018,
  author    = {Sandro Skansi},
  date      = {2018},
  title     = {Introduction to Deep Learning},
  doi       = {10.1007/978-3-319-73004-2},
  publisher = {Springer International Publishing},
  file      = {:DL/Skansi2018 - Introduction to Deep Learning.pdf:PDF},
  groups    = {DL},
  priority  = {prio1},
}

@Misc{Brendel2019,
  author    = {Brendel, Wieland and Bethge, Matthias},
  date      = {2019},
  title     = {Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet},
  doi       = {10.48550/ARXIV.1904.00760},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file      = {:CNN/Brendel2019 - Approximating CNNs with Bag of Local Features Models Works Surprisingly Well on ImageNet.pdf:PDF},
  groups    = {CNN},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences},
  publisher = {arXiv},
}

@Book{Ertel2017,
  author    = {Wolfgang Ertel},
  date      = {2017},
  title     = {Introduction to Artificial Intelligence},
  doi       = {10.1007/978-3-319-58487-4},
  publisher = {Springer International Publishing},
  file      = {:ML&AI/Ertel2017 - Introduction to Artificial Intelligence.pdf:PDF},
  groups    = {ML&AI},
  issn      = {1863-7310},
}

@Book{Kubat2017,
  author    = {Kubat, Miroslav},
  date      = {2017-08-31},
  title     = {An Introduction to Machine Learning},
  doi       = {10.1007/978-3-319-63913-0},
  isbn      = {9783319639130},
  pagetotal = {348},
  publisher = {Springer-Verlag GmbH},
  ean       = {9783319639130},
  file      = {:ML&AI/Kubat2017 - An Introduction to Machine Learning.pdf:PDF},
  groups    = {ML&AI},
  year      = {2017},
}

@Book{Aggarwal2018,
  author    = {Charu C. Aggarwal},
  date      = {2018},
  title     = {Neural Networks and Deep Learning},
  doi       = {10.1007/978-3-319-94463-0},
  publisher = {Springer International Publishing},
  file      = {:DL/Aggarwal2018 - Neural Networks and Deep Learning.pdf:PDF},
  groups    = {DL},
}

@Article{Alzubaidi2021,
  author       = {Laith Alzubaidi},
  date         = {2021},
  journaltitle = {Journal of Big Data},
  title        = {Review of deep learning: Concepts, CNN architectures, challenges, applications, future directions},
  doi          = {10.1186/s40537-021-00444-8},
  number       = {8},
  abstract     = {Journal of Big Data, https://doi.org/10.1186/s40537-021-00444-8},
  file         = {:CNN/Alzubaidi2021 - Review of Deep Learning_ Concepts, CNN Architectures, Challenges, Applications, Future Directions.pdf:PDF},
  groups       = {CNN},
  keywords     = {Deep learning, Machine learning, Convolution neural network (CNN), Deep neural network architectures, Deep learning applications, Image classification, Transfer learning, Medical image analysis, Supervised learning, FPGA, GPU},
}

@Article{Sarker2021,
  author       = {Iqbal H. Sarker},
  date         = {2021},
  journaltitle = {Springer Nature Computer Science},
  title        = {Machine Learning: Algorithms, Real-World Applications and Research Directions},
  doi          = {10.1007/s42979-021-00592-x},
  abstract     = {SN Computer Science, https://doi.org/10.1007/s42979-021-00592-x},
  file         = {:ML&AI/Sarker2021 - Machine Learning_ Algorithms, Real World Applications and Research Directions.pdf:PDF},
  groups       = {ML&AI},
  keywords     = {Machine learning, Deep learning, Artificial intelligence, Data science, Data-driven decision-making, Predictive analytics, Intelligent applications},
}

@Book{Alpaydin2010,
  author    = {Ethem Alpaydin},
  date      = {2010},
  title     = {Introduction to Machine Learning},
  isbn      = {9780262012430},
  publisher = {MIT Press},
  file      = {:ML&AI/Alpaydin2010 - Introduction to Machine Learning.pdf:PDF},
  groups    = {ML&AI},
}

@Article{Hinton2012,
  author       = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-Rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  date         = {2012},
  journaltitle = {{IEEE} Signal processing magazine},
  title        = {Deep Neural Networks for Acoustic Modeling in Speech Recognition},
  abstract     = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feedforward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.},
  file         = {:DL/Hinton2012 - Deep Neural Networks for Acoustic Modeling in Speech Recognition.pdf:PDF},
  groups       = {DL},
}

@InProceedings{Krijthe2016,
  author    = {Jesse H. Krijthe and Marco Loog},
  booktitle = {S+SSPR 2016},
  date      = {2016},
  title     = {The Peaking Phenomenon in Semi-supervised Learning},
  doi       = {10.1007/978-3-319-49055-7_27},
  editor    = {©c Springer and International Publishing and AG and A. Robles-Kelly and others},
  pages     = {299–309},
  publisher = {Springer},
  series    = {LNCS},
  volume    = {10029},
  abstract  = {For the supervised least squares classifier, when the number of training objects is smaller than the dimensionality of the data, adding more data to the training set may first increase the error rate before decreasing it. This, possibly counterintuitive, phenomenon is known as peaking. In this work, we observe that a similar but more pronounced version of this phenomenon also occurs in the semi-supervised setting, where instead of labeled objects, unlabeled objects are added to the training set. We explain why the learning curve has a more steep incline and a more gradual decline in this setting through simulation studies and by applying an approximation of the learning curve based on the work by Raudys and Duin.},
  file      = {:DL/Krijthe2016 - The Peaking Phenomenon in Semi Supervised Learning.pdf:PDF},
  groups    = {DL},
  issn      = {0302-9743},
  keywords  = {Semi-supervised learning, Peaking, Least squares classifier, Pseudo-inverse},
}

@Article{Isensee2021,
  author       = {Fabian Isensee and Paul F. Jaeger and Simon A. A. Kohl and Jens Petersen and Klaus H. Maier-Hein},
  date         = {2021},
  journaltitle = {Nature Methods},
  title        = {{nnU}-Net: a self-configuring method for deep learning-based biomedical image segmentation},
  doi          = {10.1038/s41592-020-01008-z},
  issn         = {1548-7091},
  pages        = {203-211},
  volume       = {18},
  abstract     = {Nature Methods, doi:10.1038/s41592-020-01008-z},
  file         = {:CNN/Isensee2021 - NnU Net_ a Self Configuring Method for Deep Learning Based Biomedical Image Segmentation.pdf:PDF},
  groups       = {CNN},
  priority     = {prio1},
}

@InProceedings{Sutskever2014,
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc},
  booktitle = {Advances in neural information processing systems},
  date      = {2014-12-14},
  title     = {Sequence to Sequence Learning with Neural Networks},
  eprint    = {arXiv:1409.3215v3[cs.CL]},
  volumes   = {27},
  abstract  = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  day       = {14},
  file      = {:DL/Sutskever2014 - Sequence to Sequence Learning with Neural Networks.pdf:PDF},
  groups    = {DL},
  month     = {12},
  year      = {2014},
}

@InProceedings{Krizhevsky2010,
  author    = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
  booktitle = {Advances in Neural Information Processing Systems},
  date      = {2010},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  editor    = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
  publisher = {Curran Associates, Inc.},
  file      = {:DL/Krizhevsky2010 - ImageNet Classification with Deep Convolutional Neural Networks.pdf:PDF},
  groups    = {DL},
}

@Article{Hinton2006,
  author       = {Hinton, Geoffrey and Osindero, Simon and Teh, Yee-Whye},
  date         = {2006},
  journaltitle = {Neural Computation},
  title        = {A Fast Learning Algorithm for Deep Belief Nets},
  number       = {7},
  volume       = {18},
  abstract     = {We show how to use "complementary priors" to eliminate the explainingaway effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
  file         = {:DL/Hinton2006 - A Fast Learning Algorithm for Deep Belief Nets.pdf:PDF},
  groups       = {DL},
}

@InCollection{Mathew2021,
  author    = {Amitha Mathew and P. Amudha and S. Sivakumari},
  booktitle = {Advances in Intelligent Systems and Computing},
  date      = {2021},
  title     = {Deep Learning Techniques: An Overview},
  doi       = {10.1007/978-981-15-3383-9_54},
  pages     = {599--608},
  publisher = {Springer Singapore},
  abstract  = {Deep Learning is a class of machine learning which performs much better on unstructured data. Deep learning techniques are outperforming current machine learning techniques. It enables computational models to learn features progressively from data at multiple levels. The popularity of deep learning amplified as the amount of data available increased as well as the advancement of hardware that provides powerful computers. This article comprises of the evolution of deep learning, various approaches to deep learning, architectures of deep learning, methods, and applications.},
  file      = {:DL/Mathew2021 - Deep Learning Techniques_ an Overview.pdf:PDF},
  groups    = {DL},
  keywords  = {Deep Learning (DL), Recurrent Neural Network (RNN), Deep Belief Networks (DBN), Convolutional Neural Networks(CNN), Generative Adversarial Networks(GAN)},
}

@InProceedings{Olazaran1993,
  author    = {Olazaran, Mlkel},
  booktitle = {Advances in Computers},
  date      = {1993-08},
  title     = {A Sociological History of the Neural Network Controversy},
  abstract  = {335 338 346 350 355 368 370 375 386 390 396 406 41 1 41 1 416 417 418 418 419 419 1. Introduction: A Sociological View of Scientific Controversies Neural networks, also called artificial neural networks, connectionist networks, parallel distributed systems, and neural computing systems, are information-processing systems composed of many interconnected processing units (simplified neurons) that interact in a parallel fashion to produce a result or output. The massively parallel architecture of these systems is remarkably different from that of a conventional von Neumann digital},
  file      = {:DL/Olazaran1993 - A Sociological History of the Neural Network Controversy.pdf:PDF},
  groups    = {DL},
  keywords  = {connectionism},
}

@Article{Olazaran1996,
  author       = {Olazaran, Mikel},
  date         = {1996},
  journaltitle = {Social Studies of Science},
  title        = {A Sociological Study of the Official History of the Perceptrons Controversy},
  number       = {3},
  volume       = {26},
  file         = {:DL/Olazaran1996 - A Sociological Study of the Official History of the Perceptrons Controversy.pdf:PDF},
  groups       = {DL},
}

@Article{Rosenblatt1957,
  author       = {Rosenblatt, F.},
  date         = {1957},
  journaltitle = {Psychological Review},
  title        = {THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN 1},
  number       = {6},
  volume       = {65},
  file         = {:DL/Rosenblatt1957 - THE PERCEPTRON_ a PROBABILISTIC MODEL fOR INFORMATION STORAGE aND ORGANIZATION iN tHE BRAIN 1.pdf:PDF},
  groups       = {DL},
}

@Book{Minsky1988,
  author    = {Marvin Minsky and Seymour A. Papert},
  date      = {1988},
  title     = {Perceptrons: An Introduction to Computational Geometry},
  publisher = {Cambridge, Mass. : MIT Press},
  url       = {https://archive.org/details/perceptronsintro00mins/page/52/mode/2up},
  groups    = {ML&AI},
}

@Article{Fukushima1980,
  author       = {Fukushima, Kunihiko},
  date         = {1980},
  journaltitle = {Biological Cybernetics},
  title        = {Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  doi          = {10.1007/bf00344251},
  issn         = {0340-1200},
  pages        = {193-202},
  volume       = {36},
  abstract     = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by "learning without a teacher", and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname "neocognitron". After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consists of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of "S-cells', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of "C-cells" similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any "teacher" during the process of selforganization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cells of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  file         = {:CNN/Fukushima1980 - Neocognitron_ a Self Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position.pdf:PDF},
  groups       = {CNN},
}

@InBook{Rumelhart1988,
  author    = {D. E. Rumelhart and G. E. Hinton and R. J. Williams},
  booktitle = {Readings in Cognitive Science},
  date      = {1988},
  title     = {Learning Internal Representations by Error Propagation},
  doi       = {10.1016/b978-1-4832-1446-7.50035-2},
  pages     = {399-421},
  publisher = {Elsevier},
  file      = {:DL/Rumelhart1988 - Learning Internal Representations by Error Propagation.pdf:PDF},
  groups    = {DL},
}

@InProceedings{Dike2018,
  author    = {Happiness Ugochi Dike and Yimin Zhou and Kranthi Kumar Deveerasetty and Qingtian Wu},
  booktitle = {2018 {IEEE} International Conference on Cyborg and Bionic Systems ({CBS})},
  date      = {2018-10},
  title     = {Unsupervised Learning Based On Artificial Neural Network: A Review},
  doi       = {10.1109/cbs.2018.8612259},
  publisher = {{IEEE}},
  abstract  = {Artificial neural networks (ANN) have been applied effectively in numerous fields for the aim of prediction, knowledge discovery, classification, time series analysis, modeling, etc. ANN training can be assorted into Supervised learning, Reinforcement learning and Unsupervised learning. There are some limitations using supervised learning. These limitations can be overcome by using unsupervised learning technique. This gives us motivation to write a review on unsupervised learning based on ANN. One main problem associated with unsupervised learning is how to find the hidden structures in unlabeled data. This paper reviews on the training/learning of unsupervised learning based on artificial neural network. It provides a description of the methods of selecting and fixing a number of hidden nodes in an unsupervised learning environment based on ANN. Moreover, the status, benefits and challenges of unsupervised learning are also summarized.},
  file      = {:DL/Dike2018 - Unsupervised Learning Based on Artificial Neural Network_ a Review.pdf:PDF},
  groups    = {DL},
  keywords  = {Artificial Neural Network, Unsupervised learning, Training, Hidden nodes},
}

@Article{Reinke2022,
  author       = {Reinke, Annika and Bankhead, Peter and Cheplygina, Veronika and Hashimoto, Daniel and Hoffman, Michael and Karthikesalingam, Alan and Kainz, Bernhard and Kreshuk, Anna and Landman, Bennett and Maier-Hein, Klaus and Martel, Anne and Mattson, Peter and Moher, David and Moons, Karel and Nickel, Felix and Petersen, Jens and Reyes, Mauricio and Rieke, Nicola and Gmbh, Nvidia and Germany, Michael and Riegler and Summers, Ronald and Taha, Abdel and Austria, Sotirios and Tsaftaris and Yaniv, Ziv and Maier-Hein, Lena},
  date         = {2022},
  journaltitle = {Computing Research Repository},
  title        = {Common Limitations of Image Processing Metrics: A Picture Story},
  file         = {:Segmentation/Reinke2022 - Common Limitations of Image Processing Metrics_ a Picture Story.pdf:PDF},
  groups       = {Segmentation},
  year         = {2022},
}

@Article{Kennedy2013,
  author       = {Mary B. Kennedy},
  date         = {2013-12},
  journaltitle = {Cold Spring Harbor Perspectives in Biology},
  title        = {Synaptic Signaling in Learning and Memory},
  doi          = {10.1101/cshperspect.a016824},
  number       = {2},
  pages        = {a016824},
  volume       = {8},
  abstract     = {Learning and memory require the formation of new neural networks in the brain. A key mechanism underlying this process is synaptic plasticity at excitatory synapses, which connect neurons into networks. Excitatory synaptic transmission happens when glutamate, the excitatory neurotransmitter, activates receptors on the postsynaptic neuron. Synaptic plasticity is a higher-level process in which the strength of excitatory synapses is altered in response to the pattern of activity at the synapse. It is initiated in the postsynaptic compartment, where the precise pattern of influx of calcium through activated glutamate receptors leads either to the addition of new receptors and enlargement of the synapse (long-term potentiation) or the removal of receptors and shrinkage of the synapse (long-term depression). Calcium/calmodulin-regulated enzymes and small GTPases collaborate to control this highly tuned mechanism.},
  file         = {:Neural Networks/Kennedy2013 - Synaptic Signaling in Learning and Memory.pdf:PDF},
  groups       = {Neural Networks},
  publisher    = {Cold Spring Harbor Laboratory},
}

@Article{Pouliakis2016,
  author       = {Abraham Pouliakis and Efrossyni Karakitsou and Niki Margari and Panagiotis Bountris and Maria Haritou and John panayiotides and Dimitrios Koutsouris and Petros Karakitsos},
  date         = {2016},
  journaltitle = {Biomedical Engineering and Computational Biology},
  title        = {Artificial Neural Networks as Decision Support Tools in Cytopathology: Past, Present, and Future},
  doi          = {10.4137/becb.s31601},
  volume       = {7},
  abstract     = {This study aims to analyze the role of artificial neural networks (ANNs) in cytopathology. More specifically, it aims to highlight the importance of employing ANNs in existing and future applications and in identifying unexplored or poorly explored research topics. STUDY DESIGN: A systematic search was conducted in scientific databases for articles related to cytopathology and ANNs with respect to anatomical places of the human body where cytopathology is performed. For each anatomic system/organ, the major outcomes described in the scientific literature are presented and the most important aspects are highlighted. RESULTS: The vast majority of ANN applications are related to cervical cytopathology, specifically for the ANN-based, semiautomated commercial diagnostic system PAPNET. For cervical cytopathology, there is a plethora of studies relevant to the diagnostic accuracy; in addition, there are also efforts evaluating cost-effectiveness and applications on primary, secondary, or hybrid screening. For the rest of the anatomical sites, such as the gastrointestinal system, thyroid gland, urinary tract, and breast, there are significantly less efforts relevant to the application of ANNs. Additionally, there are still anatomical systems for which ANNs have never been applied on their cytological material. CONCLUSIONS: Cytopathology is an ideal discipline to apply ANNs. In general, diagnosis is performed by experts via the light microscope. However, this approach introduces subjectivity, because this is not a universal and objective measurement process. This has resulted in the existence of a gray zone between normal and pathological cases. From the analysis of related articles, it is obvious that there is a need to perform more thorough analyses, using extensive number of cases and particularly for the nonexplored organs. Efforts to apply such systems within the laboratory test environment are required for their future uptake.},
  file         = {:Neural Networks/Pouliakis2016 - Artificial Neural Networks As Decision Support Tools in Cytopathology_ Past, Present, and Future.pdf:PDF},
  groups       = {Neural Networks},
  keywords     = {artificial neural networks, neural networks, artificial intelligence, cytopathology, cytology, review, automation, computer-assisted diagnosis, decision support},
  publisher    = {{SAGE} Publications},
}

@Article{Sharma2020,
  author       = {Siddharth Sharma and Simone Sharma and Anidhya Athaiya},
  date         = {2020},
  journaltitle = {International Journal of Engineering Applied Sciences and Technology},
  title        = {ACTIVATION FUNCTIONS IN NEURAL NETWORKS},
  doi          = {10.33564/ijeast.2020.v04i12.054},
  issn         = {2455-2143},
  volume       = {04},
  abstract     = {Artificial Neural Networks are inspired from the human brain and the network of neurons present in the brain. The information is processed and passed on from one neuron to another through neuro synaptic junctions. Similarly, in artificial neural networks there are different layers of cells arranged and connected to each other. The output/information from the inner layers of the neural network are passed on to the next layers and finally to the outermost layer which gives the output. The input to the outer layer is provided nonlinearity to inner layers' output so that it can be further processed. In an Artificial Neural Network, activation functions are very important as they help in learning and making sense of non-linear and complicated mappings between the inputs and corresponding outputs.},
  file         = {:Neural Networks/Sharma2020 - ACTIVATION FUNCTIONS iN NEURAL NETWORKS.pdf:PDF},
  groups       = {Neural Networks},
}

@Article{Shrestha2019,
  author       = {Ajay Shrestha and Ausif Mahmood},
  date         = {2019},
  journaltitle = {{IEEE} Access},
  title        = {Review of Deep Learning Algorithms and Architectures},
  doi          = {10.1109/access.2019.2912200},
  note         = {overview of DL techniques!},
  pages        = {53040--53065},
  volume       = {7},
  abstract     = {Deep learning (DL) is playing an increasingly important role in our lives. It has already made a huge impact in areas, such as cancer diagnosis, precision medicine, self-driving cars, predictive forecasting, and speech recognition. The painstakingly handcrafted feature extractors used in traditional learning, classification, and pattern recognition systems are not scalable for large-sized data sets. In many cases, depending on the problem complexity, DL can also overcome the limitations of earlier shallow networks that prevented efficient training and abstractions of hierarchical representations of multi-dimensional training data. Deep neural network (DNN) uses multiple (deep) layers of units with highly optimized algorithms and architectures. This paper reviews several optimization methods to improve the accuracy of the training and to reduce training time. We delve into the math behind training algorithms used in recent deep networks. We describe current shortcomings, enhancements, and implementations. The review also covers different types of deep architectures, such as deep convolution networks, deep residual networks, recurrent neural networks, reinforcement learning, variational autoencoders, and others.},
  file         = {:DL/Shrestha2019 - Review of Deep Learning Algorithms and Architectures.pdf:PDF},
  groups       = {DL},
  keywords     = {INDEX TERMS Machine learning algorithm, optimization, artificial intelligence, deep neural network architectures, convolution neural network, backpropagation, supervised and unsupervised learning},
  publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Schmidhuber2014,
  author       = {Schmidhuber, Jürgen},
  date         = {2014-10-08},
  journaltitle = {Computing Research Repository},
  title        = {Deep Learning in Neural Networks: An Overview},
  eprint       = {arXiv:1404.7828v4[cs.NE]},
  abstract     = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  comment      = {neuroscience and NN},
  day          = {8},
  file         = {:DL/Schmidhuber2014 - Deep Learning in Neural Networks_ an Overview.pdf:PDF},
  groups       = {DL},
  month        = {10},
  year         = {2014},
}

@Article{Voulodimos2018,
  author       = {Athanasios Voulodimos and Nikolaos Doulamis and Anastasios Doulamis and Eftychios Protopapadakis},
  date         = {2018-02-01},
  journaltitle = {Computational Intelligence and Neuroscience},
  title        = {Deep Learning for Computer Vision: A Brief Review},
  doi          = {10.1155/2018/7068349},
  editor       = {Diego Andina},
  number       = {Greece},
  pages        = {1--13},
  volume       = {2018},
  abstract     = {Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.},
  day          = {1},
  file         = {:DL/Voulodimos2018 - Deep Learning for Computer Vision_ a Brief Review.pdf:PDF},
  groups       = {DL},
  month        = {2},
  publisher    = {Hindawi Limited},
  year         = {2018},
}

@Article{AbdelJaber2022,
  author       = {Hussein Abdel-Jaber and Disha Devassy and Azhar Al Salam and Lamya Hidaytallah and Malak EL-Amir},
  date         = {2022-02-21},
  journaltitle = {Algorithms},
  title        = {A Review of Deep Learning Algorithms and Their Applications in Healthcare},
  doi          = {10.3390/a15020071},
  editor       = {Frank Werner},
  number       = {2},
  pages        = {71},
  volume       = {15},
  abstract     = {This article is an open access article distributed under the terms and conditions of the Creative Commons Attribution (CC BY},
  comment      = {goog overview & classification of supervised and unsupervised techniques},
  day          = {21},
  file         = {:DL/AbdelJaber2022 - A Review of Deep Learning Algorithms and Their Applications in Healthcare.pdf:PDF},
  groups       = {DL},
  keywords     = {Abdel-Jaber, H., Devassy, D., Al Salam, A., Hidaytallah, L., Algorithms 2022, 15, 71. https:// artificial neural networks (ANN), deep learning, autoencoders (AE), convolutional neural networks (CNN), recurrent neural network (RNN), health care sector},
  month        = {2},
  publisher    = {{MDPI} {AG}},
  year         = {2022},
}

@Article{Mishra2020,
  author       = {Manohar Mishra and Janmenjoy Nayak and Bighnaraj Naik and Ajith Abraham},
  date         = {2020-11-09},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  title        = {Deep learning in electrical utility industry: A comprehensive review of a decade of research},
  doi          = {10.1016/j.engappai.2020.104000},
  number       = {India},
  pages        = {104000},
  volume       = {96},
  abstract     = {Smart-grid (SG) is a new revolution in the electrical utility industry (EUI) over the past decade. With each moving day, some new advanced technologies are coming into the picture which forces the utility engineers to think about its application to make the electrical grid become smarter. Artificial intelligence (AI) techniques such as machine learning (ML), artificial neural network (ANN), deep learning (DL), reinforcement learning (RL), and deep-reinforcement learning (DRL) are the few examples of above-mentioned advanced technologies by which large volume of collected information being processed, and deliver the solution to the complex problems associated with EUI. In recent times, DL for artificial intelligence applications has gained huge attention in the diverse research area. The traditional ML techniques have several constrained for processing the data in raw form. However, the DL provides the options to process the raw data without extracting and selecting the feature vector. The DL techniques belong to a new era of AI development. This article presents the taxonomy of DL algorithms available in the literature applied to different problems in EUI. The main objective of this survey is to provide a comprehensive idea to the researcher/utility engineer about the applications and future research scope of DL methods for power systems studies.},
  comment      = {good overview on DL techniques! (+ comparison and strengths)},
  day          = {9},
  file         = {:DL/Mishra2020 - Deep Learning in Electrical Utility Industry_ a Comprehensive Review of a Decade of Research.pdf:PDF},
  groups       = {DL},
  month        = {11},
  publisher    = {Elsevier {BV}},
  year         = {2020},
}

@InProceedings{Karras2019,
  author    = {Tero Karras and Samuli Laine and Timo Aila},
  booktitle = {Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition},
  date      = {2019},
  title     = {A Style-Based Generator Architecture for Generative Adversarial Networks},
  doi       = {10.1109/cvpr.2019.00453},
  abstract  = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  file      = {:DL/Karras2019 - A Style Based Generator Architecture for Generative Adversarial Networks.pdf:PDF},
  groups    = {DL},
}

@Article{Moubayed2018,
  author       = {Abdallah Moubayed and Mohammadnoor Injadat and Ali Bou Nassif and Hanan Lutfiyya and Abdallah Shami},
  date         = {2018},
  journaltitle = {{IEEE} Access},
  title        = {E-Learning: Challenges and Research Opportunities Using Machine Learning and Data Analytics},
  doi          = {10.1109/access.2018.2851790},
  pages        = {39117--39138},
  volume       = {6},
  abstract     = {With the proliferation of technology, the field of elearning has garnered significant attention in recent times. This is because it has allowed users from around the world to learn and access new information. This has added to the growing amount of collected data that is already being generated through different devices and sensors employed around the world. This has led to the need to analyze collected data and extract useful information from it. Machine learning and data analytics are proposed techniques that can help extract information and find valuable patterns within the collected data. In this work, the field of e-learning is investigated in terms of definitions and characteristics. Moreover, the various challenges facing the different participants within this process are discussed. Also some of the works proposed in the literature to tackle these challenges is presented. Then, a brief survey about some of the most popular machine learning and data analytics techniques is given. Finally, some of the research opportunities available that employ such techniques are proposed to give insights into the areas that merit further exploration and investigation.},
  file         = {:ML&AI/Moubayed2018 - E Learning_ Challenges and Research Opportunities Using Machine Learning and Data Analytics.pdf:PDF},
  groups       = {ML&AI},
  keywords     = {e-Learning, Machine Learning, Data Analytics},
  publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Article{Johnson2020,
  author       = {N. S. Johnson and P. S. Vulimiri and A. C. To and X. Zhang and C. A. Brice and B. B. Kappes and A. P. Stebner},
  date         = {2020},
  journaltitle = {Additive Manufacturing},
  title        = {Invited review: Machine learning for materials developments in metals additive manufacturing},
  doi          = {10.1016/j.addma.2020.101641},
  issn         = {2214-8604},
  pages        = {101641},
  volume       = {36},
  file         = {:Neural Networks/Johnson2020 - Invited Review_ Machine Learning for Materials Developments in Metals Additive Manufacturing.pdf:PDF},
  groups       = {Neural Networks},
}

@Article{Ouali2020,
  author       = {Ouali, Yassine and Hudelot, Céline and Tami, Myriam},
  date         = {2020-07-06},
  journaltitle = {Computing Research Repository},
  title        = {An Overview of Deep Semi-Supervised Learning},
  eprint       = {arXiv:2006.05278v2[cs.LG]},
  abstract     = {Deep neural networks demonstrated their ability to provide remarkable performances on a wide range of supervised learning tasks (e.g., image classification) when trained on extensive collections of labeled data (e.g., ImageNet). However, creating such large datasets requires a considerable amount of resources, time, and effort. Such resources may not be available in many practical cases, limiting the adoption and the application of many deep learning methods. In a search for more data-efficient deep learning methods to overcome the need for large annotated datasets, there is a rising research interest in semi-supervised learning and its applications to deep neural networks to reduce the amount of labeled data required, by either developing novel methods or adopting existing semi-supervised learning frameworks for a deep learning setting. In this paper, we provide a comprehensive overview of deep semi-supervised learning, starting with an introduction to the field, followed by a summarization of the dominant semi-supervised approaches in deep learning 1 .},
  day          = {6},
  file         = {:DL/Ouali2020 - An Overview of Deep Semi Supervised Learning.pdf:PDF},
  groups       = {DL},
  keywords     = {semi-supervised learning, deep learning, neural networks, consistency training, entropy minimization, proxy labeling, generative models, graph neural networks},
  month        = {7},
  year         = {2020},
}

@Article{Arulkumaran2017,
  author       = {Kai Arulkumaran and Marc Peter Deisenroth and Miles Brundage and Anil Anthony Bharath},
  date         = {2017},
  journaltitle = {{IEEE} Signal processing magazine},
  title        = {Deep Reinforcement Learning: A Brief Survey},
  doi          = {10.1109/msp.2017.2743240},
  issn         = {1053-5888},
  pages        = {26-38},
  volume       = {34},
  abstract     = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policybased methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep Q-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  file         = {:DL/Arulkumaran2017 - Deep Reinforcement Learning_ a Brief Survey.pdf:PDF},
  groups       = {DL},
  publisher    = {IEEE},
}

@Article{Hubel1962,
  author       = {D. H. Hubel and T. N. Wiesel},
  date         = {1962},
  journaltitle = {The Journal of Physiology},
  title        = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex},
  doi          = {10.1113/jphysiol.1962.sp006837},
  issn         = {0022-3751},
  pages        = {106-154},
  volume       = {160},
  file         = {:CNN/Hubel1962 - Receptive Fields, Binocular Interaction and Functional Architecture in the Cat's Visual Cortex.pdf:PDF},
  groups       = {CNN},
}

@Article{Tafforeau2006,
  author       = {P. Tafforeau and R. Boistel and E. Boller and A. Bravin and M. Brunet and Y. Chaimanee and P. Cloetens and M. Feist and J. Hoszowska and J.-J. Jaeger and R. F. Kay and V. Lazzari and L. Marivaux and A. Nel and C. Nemoz and X. Thibault and P. Vignaud and S. Zabler},
  date         = {2006},
  journaltitle = {Applied Physics A},
  title        = {Applications of X-ray synchrotron microtomography for non-destructive 3D studies of paleontological specimens},
  doi          = {10.1007/s00339-006-3507-2},
  issn         = {0947-8396},
  volume       = {83},
  abstract     = {Paleontologists are quite recent newcomers among important data, for example about teeth structure and develop-
the users of X-ray synchrotron imaging techniques at the Eu-
ropean Synchrotron Radiation Facility (ESRF). Studies of the ment [1–3]. In some cases (i.e. for a majority of microfossils)},
  file         = {:Segmentation/Tafforeau2006 - Applications of X Ray Synchrotron Microtomography for Non Destructive 3D Studies of Paleontological Specimens.pdf:PDF},
  groups       = {Segmentation},
}

@Article{Antonelli2022,
  author       = {M. Antonelli and Annika Reinke and S. Bakas and K. Farahani and AnnetteKopp-Schneider and B. Landman and G. Litjens and B. Menze and O. Ronneberger and Ronald M.Summers and B. Ginneken and M. Bilello and Patrick Bilic and P. Christ and Richard K. G. Do and M. Gollub and S. Heckers and H. Huisman and W. Jarnagin and M. McHugo and S. Napel and Jennifer S. Goli Pernicka and K. Rhode and C. Tobon-Gomez and Eugene Vorontsov and J. Meakin and S. Ourselin and M. Wiesenfarth and P. Arbeláez and Byeonguk Bae and Sihong Chen and L. Daza and Jian-Jun Feng and Baochun He and F. Isensee and Yuanfeng Ji and F. Jia and Namkug Kim and Ildoo Kim and D. Merhof and A. Pai and Beomhee Park and Mathias Perslev and R. Rezaiifar and Oliver Rippel and Ignacio Sarasua and Wei Shen and Jaemin Son and C. Wachinger and Liansheng Wang and Yan Wang and Yingda Xia and Daguang Xu and Zhanwei Xu and Yefeng Zheng and Amber L. Simpson and L. Maier-Hein and M. Cardoso},
  date         = {2022},
  journaltitle = {Nature Communications},
  title        = {The Medical Segmentation Decathlon},
  doi          = {10.1038/s41467-022-30695-9},
  number       = {1},
  volume       = {13},
  abstract     = {International challenges have become the de facto standard for comparative assessment of image analysis algorithms given a specific task. Segmentation is so far the most widely investigated medical image processing task, but the various segmentation challenges have typically been organized in isolation, such that algorithm development was driven by the need to tackle a single specific clinical problem. We hypothesized that a method capable of performing well on multiple tasks will generalize well to a previously unseen task and potentially outperform a custom-designed solution. To investigate the hypothesis, we organized the Medical Segmentation Decathlon (MSD)—a biomedical image analysis challenge, in which algorithms compete in a multitude of both tasks and modalities. The underlying data set was designed to explore the axis of difficulties typically encountered when dealing with medical images, such as small data sets, unbalanced labels, multi-site data and small objects. The MSD challenge confirmed that algorithms with a consistent good performance on a set of tasks preserved their good average performance on a different set of previously unseen tasks. Moreover, by monitoring the MSD winner for two years, we found that this algorithm continued generalizing well to a wide range of other clinical problems, further confirming our hypothesis. Three main conclusions can be drawn from this study: (1) state-of-the-art image segmentation algorithms are mature, accurate, and generalize well when retrained on unseen tasks; (2) consistent algorithmic performance across multiple tasks is a strong surrogate of algorithmic generalizability; (3) the training of accurate AI segmentation models is now commoditized to non AI experts.},
  file         = {:Segmentation/Antonelli2022 - The Medical Segmentation Decathlon.pdf:PDF},
  groups       = {Segmentation},
}

@Article{Sarker2021a,
  author       = {Iqbal H. Sarker},
  date         = {2021},
  journaltitle = {SN Computer Science},
  title        = {Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions},
  doi          = {10.1007/s42979-021-00815-1},
  abstract     = {SN Computer Science, https://doi.org/10.1007/s42979-021-00815-1},
  file         = {:DL/Sarker2021a - Deep Learning_ a Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions.pdf:PDF},
  groups       = {DL},
  keywords     = {Deep learning, Artificial neural network, Artificial intelligence, Discriminative learning, Generative learning, Hybrid learning, Intelligent systems},
  priority     = {prio1},
}

@InProceedings{Huang2015,
  author    = {Jie Huang and Wengang Zhou and Houqiang Li and Weiping Li},
  booktitle = {2015 {IEEE} International Conference on Multimedia and Expo ({ICME})},
  date      = {2015-06},
  title     = {Sign Language Recognition using 3D convolutional neural networks},
  doi       = {10.1109/icme.2015.7177428},
  number    = {auto-},
  pages     = {1-6},
  publisher = {{IEEE}},
  abstract  = {Sign Language Recognition (SLR) targets on interpreting the sign language into text or speech, so as to facilitate the communication between deaf-mute people and ordinary people. This task has broad social impact, but is still very challenging due to the complexity and large variations in hand actions. Existing methods for SLR use hand-crafted features to describe sign language motion and build classification models based on those features. However, it is difficult to design reliable features to adapt to the large variations of hand gestures. To approach this problem, we propose a novel 3D convolutional neural network (CNN) which extracts discriminative spatial-temporal features from raw video stream automatically without any prior knowledge, avoiding designing features. To boost the performance, multi-channels of video streams, including color information, depth clue, and body joint positions, are used as input to the 3D CNN in order to integrate color, depth and trajectory information. We validate the proposed model on a real dataset collected with Microsoft Kinect and demonstrate its effectiveness over the traditional approaches based on hand-crafted features.},
  file      = {:CNN/Huang2015 - Sign Language Recognition Using 3D Convolutional Neural Networks.pdf:PDF},
  groups    = {CNN},
  keywords  = {Sign Language Recognition, 3D Convolutional Neural Networks, Deep Learning},
}

@InCollection{Cicek2016,
  author    = {Özgün {\c{C}}i{\c{c}}ek and Ahmed Abdulkadir and Soeren S. Lienkamp and Thomas Brox and Olaf Ronneberger},
  booktitle = {Medical Image Computing and Computer-Assisted Intervention {\textendash} {MICCAI} 2016},
  date      = {2016},
  title     = {3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation},
  doi       = {10.1007/978-3-319-46723-8_49},
  pages     = {424--432},
  publisher = {Springer International Publishing},
  abstract  = {This paper introduces a network for volumetric segmentation that learns from sparsely annotated volumetric images. We outline two attractive use cases of this method: (1) In a semi-automated setup, the user annotates some slices in the volume to be segmented. The network learns from these sparse annotations and provides a dense 3D segmentation. (2) In a fully-automated setup, we assume that a representative, sparsely annotated training set exists. Trained on this data set, the network densely segments new volumetric images. The proposed network extends the previous u-net architecture from Ronneberger et al. by replacing all 2D operations with their 3D counterparts. The implementation performs on-the-fly elastic deformations for efficient data augmentation during training. It is trained end-to-end from scratch, i.e., no pre-trained network is required. We test the performance of the proposed method on a complex, highly variable 3D structure, the Xenopus kidney, and achieve good results for both use cases.},
  file      = {:CNN/Cicek2016 - 3D U Net_ Learning Dense Volumetric Segmentation from Sparse Annotation.pdf:PDF},
  groups    = {CNN},
  issn      = {0302-9743},
  keywords  = {Convolutional Neural Networks, 3D, Biomedical Volumetric Image Segmentation, Xenopus Kidney, Semi-automated, Fully-automated, Sparse Annotation},
}

@Article{Ruder2017,
  author       = {Sebastian Ruder},
  date         = {2017},
  journaltitle = {arXiv preprint},
  title        = {An overview of gradient descent optimization algorithms},
  eprint       = {1609.04747},
  eprinttype   = {arXiv},
  url          = {https://www.semanticscholar.org/paper/769ef3d5021cd71c37d2c403f231a53d1accf786},
  abstract     = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  file         = {:Neural Networks/Ruder2017 - An Overview of Gradient Descent Optimization Algorithms.pdf:PDF},
  groups       = {Neural Networks},
  venue        = {ArXiv},
  year         = {2017},
}

@InProceedings{Jadon2020,
  author    = {Jadon, Shruti},
  booktitle = {2020 {IEEE} Conference on Computational Intelligence in Bioinformatics and Computational Biology ({CIBCB})},
  date      = {2020-10},
  title     = {A survey of loss functions for semantic segmentation},
  doi       = {10.1109/cibcb48159.2020.9277638},
  pages     = {1-7},
  publisher = {{IEEE}},
  abstract  = {Image Segmentation has been an active field of research as it has a wide range of applications, ranging from automated disease detection to self driving cars. In the past 5 years, various papers came up with different objective loss functions used in different cases such as biased data, sparse segmentation, etc. In this paper, we have summarized some of the well-known loss functions widely used for Image Segmentation and listed out the cases where their usage can help in fast and better convergence of a model. Furthermore, we have also introduced a new log-cosh dice loss function and compared its performance on NBFS skull-segmentation open source data-set with widely used loss functions. We also showcased that certain loss functions perform well across all data-sets and can be taken as a good baseline choice in unknown data distribution scenarios.},
  file      = {:Segmentation/Jadon2020 - A Survey of Loss Functions for Semantic Segmentation.pdf:PDF},
  groups    = {Segmentation},
  keywords  = {Computer Vision, Image Segmentation, Medical Image, Loss Function, Optimization, Healthcare, Skull Stripping, Deep Learning},
}

@InProceedings{Mishra2019,
  author    = {Purnendu Mishra and Kishor Sarawadekar},
  booktitle = {{TENCON} 2019 - 2019 {IEEE} Region 10 Conference ({TENCON})},
  date      = {2019-10},
  title     = {Polynomial Learning Rate Policy with Warm Restart for Deep Neural Network},
  doi       = {10.1109/tencon.2019.8929465},
  publisher = {{IEEE}},
  abstract  = {Learning rate (LR) is one of the most important hyper-parameters in any deep neural network (DNN) optimization process. It controls the speed of network convergence to the point of global minima by navigation through non-convex loss surface. The performance of a DNN is affected by presence of local minima, saddle points, etc. in the loss surface. Decaying the learning rate by a factor at fixed number of epochs or exponentially is the conventional way of varying the LR. Recently, two new approaches for setting learning rate have been introduced namely cyclical learning rate and stochastic gradient descent with warm restarts. In both of these approaches, the learning rate value is varied in a cyclic pattern between two boundary values. This paper introduces another warm restart technique which is inspired by these two approaches and it uses "poly" LR policy. The proposed technique is called as polynomial learning rate with warm restart and it requires only a single warm restart. The proposed LR policy helps in faster convergence of the DNN and it has slightly higher classification accuracy. The performance of the proposed LR policy is demonstrated on CIFAR-10, CIFAR-100 and tiny ImageNet dataset with CNN, ResNets and Wide Residual Networks (WRN) architectures.},
  file      = {:Neural Networks/Mishra2019 - Polynomial Learning Rate Policy with Warm Restart for Deep Neural Network.pdf:PDF},
  groups    = {Neural Networks},
  keywords  = {Deep Neural Network, Learning Rate, SGDR, CLR, polynomial learning rate, warm restart},
}

@Article{MaierHein2018,
  author       = {Maier-Hein, Lena and Eisenmann, Matthias and Reinke, Annika and Onogur, Sinan and Stankovic, Marko and Scholz, Patrick and Arbel, Tal and Bogunović, Hrvoje and Bradley, Andrew and Carass, Aaron and Feldmann, Carolin and Frangi, Alejandro and Full, Peter and Ginneken, Bram and Hanbury, Allan and Honauer, Katrin and Kozubek, Michal and Landman, Bennett and März, Keno and Kopp-Schneider, Annette},
  date         = {2018},
  journaltitle = {Nature Communications},
  title        = {Why rankings of biomedical image analysis competitions should be interpreted with care},
  doi          = {10.1038/s41467-018-07619-7},
  volume       = {9},
  abstract     = {International challenges have become the standard for validation of biomedical image analysis methods. Given their scientific impact, it is surprising that a critical analysis of common practices related to the organization of challenges has not yet been performed. In this paper, we present a comprehensive analysis of biomedical image analysis challenges conducted up to now. We demonstrate the importance of challenges and show that the lack of quality control has critical consequences. First, reproducibility and interpretation of the results is often hampered as only a fraction of relevant information is typically provided. Second, the rank of an algorithm is generally not robust to a number of variables such as the test data used for validation, the ranking scheme applied and the observers that make the reference annotations. To overcome these problems, we recommend best practice guidelines and define open research questions to be addressed in the future.},
  file         = {:Segmentation/MaierHein2018 - Why Rankings of Biomedical Image Analysis Competitions Should Be Interpreted with Care.pdf:PDF},
  groups       = {Segmentation},
  publisher    = {Springer US},
}

@WWW{Jordan2018,
  author  = {Jeremy Jordan},
  date    = {2018-05-21},
  title   = {Semantic Segmentation},
  url     = {https://www.jeremyjordan.me/semantic-segmentation/},
  urldate = {2022-06-01},
  groups  = {Segmentation},
}

@WWW{Sharma2019,
  author  = {Pulkit Sharma},
  date    = {2019-04-01},
  editor  = {{Analytics Vidhya}},
  title   = {Computer Vision Tutorial: A Step-by-Step Introduction to Image Segmentation Techniques (Part 1)},
  url     = {https://www.analyticsvidhya.com/blog/2019/04/introduction-image-segmentation-techniques-python/},
  urldate = {2022-06-01},
  groups  = {Segmentation},
}

@Article{Vidal2005,
  author       = {F.P. Vidal and J.M. L{\'{e}}tang and G. Peix and P. Cloetens},
  date         = {2005},
  journaltitle = {Nuclear Instruments and Methods in Physics Research Section B: Beam Interactions with Materials and Atoms},
  title        = {Investigation of artefact sources in synchrotron microtomography via virtual X-ray imaging},
  doi          = {10.1016/j.nimb.2005.02.003},
  number       = {3},
  volume       = {234},
  abstract     = {Qualitative and quantitative use of volumes reconstructed by computed tomography (CT) can be compromised due to artefacts which corrupt the data. This article illustrates a method based on virtual X-ray imaging to investigate sources of artefacts which occur in microtomography using synchrotron radiation. In this phenomenological study, different computer simulation methods based on physical X-ray properties, eventually coupled with experimental data, are used in order to compare artefacts obtained theoretically to those present in a volume acquired experimentally, or to predict them for a particular experimental setup. The article begins with the presentation of a synchrotron microtomographic slice of a reinforced fibre composite acquired at the European Synchrotron Radiation Facility (ESRF) containing streak artefacts. This experimental context is used as the motive throughout the paper to illustrate the investigation of some artefact sources. First, the contribution of direct radiation is compared to the contribution of secondary radiations. Then, the effect of some methodological aspects are detailed, including under-sampling, sample and camera misalignment, sample extending outside of the field of view and photonic noise. The effect of harmonic components present in the experimental spectrum are also simulated. Afterwards, detector properties, such as its impulse response or defective pixels, are taken into account. Finally, the importance of phase contrast effects is evaluated. In the last section, this investigation is discussed by putting emphasis on the experimental context which is used throughout this paper.},
  file         = {:Segmentation/Vidal2005 - Investigation of Artefact Sources in Synchrotron Microtomography Via Virtual X Ray Imaging.pdf:PDF},
  groups       = {Segmentation},
  keywords     = {87.59.F, 07.85.Q, 07.05.T, 02.50.N, 42.15.D X-ray microtomography, Artefact, Deterministic simulation (ray-tracing), Monte Carlo method, Phase contrast, Modulation transfer function},
  publisher    = {Elsevier {BV}},
  year         = {2005},
}

@Article{Kar2021,
  author       = {Mithun Kumar Kar and Malaya Kumar Nath and Debanga Raj Neog},
  date         = {2021},
  journaltitle = {{SN} Computer Science},
  title        = {A Review on Progress in Semantic Image Segmentation and Its Application to Medical Images},
  doi          = {10.1007/s42979-021-00784-5},
  number       = {5},
  volume       = {2},
  abstract     = {Semantic image segmentation is a popular image segmentation technique where each pixel in an image is labeled with an object class. This technique has become a vital part of image analysis nowadays as it facilitates the description, categorization, and visualization of the regions of interest in an image. The recent developments in computer vision algorithms and the increasing availability of large datasets have made semantic image segmentation very popular in the field of computer vision. Motivated by the human visual system which can identify objects in a complex scene very efficiently, researchers are interested in building a model that can semantically segment an image into meaningful object classes. This paper reviews deep learning-based semantic segmentation techniques that use deep neural network architectures for image segmentation of biomedical images. We have provided a discussion on the fundamental concepts related to deep learning methods used in semantic segmentation for the benefit of readers. The standard datasets and existing deep network architectures used in both medical and non-medical fields are discussed with their significance. Finally, this paper concludes by discussing the challenges and future research directions in the field of deep learning-based semantic segmentation for applications in the medical field.},
  file         = {:Segmentation/Kar2021 - A Review on Progress in Semantic Image Segmentation and Its Application to Medical Images.pdf:PDF},
  groups       = {Segmentation},
  keywords     = {Semantic segmentation, Deep learning, Automated medical image analysis, Convolution neural network, Deep neural network, Recurrent neural network},
  publisher    = {Springer Science and Business Media {LLC}},
  year         = {2021},
}

@Book{Szeliski2021,
  author   = {Szeliski, Richard},
  date     = {2021},
  title    = {Computer Vision: Algorithms and Applications},
  abstract = {Computer Vision},
  file     = {:Segmentation/Szeliski2021 - Computer Vision_ Algorithms and Applications 2nd Edition.pdf:PDF},
  groups   = {Segmentation},
  keywords = {computer vision},
  year     = {2021},
}

@Article{Bercovich2018,
  author       = {Eyal Bercovich and Marcia C. Javitt},
  date         = {2018},
  journaltitle = {Rambam Maimonides Medical Journal},
  title        = {Medical Imaging: From Roentgen to the Digital Revolution, and Beyond},
  doi          = {10.5041/rmmj.10355},
  number       = {4},
  volume       = {9},
  abstract     = {Today medical imaging is an essential component of the entire health-care continuum, from wellness and screening, to early diagnosis, treatment selection, and follow-up. Patient triage in both acute care and chronic disease, imaging-guided interventions, and optimization of treatment planning are now integrated into routine clinical practice in all subspecialties. This paper provides a brief review of major milestones in medical imaging from its inception to date, with a few considerations regarding future directions in this important field.},
  file         = {:Segmentation/Bercovich2018 - Medical Imaging_ from Roentgen to the Digital Revolution, and beyond.pdf:PDF},
  groups       = {Segmentation},
  keywords     = {ACR, American College of Radiology, AC, Appropriateness Criteria, AI, artificial intelligence, CAD, computer-aided diagnosis, CEUS, contrast-enhanced ultrasound, CT, computed tomography, CTA, coronary CT angiography, fMRI, functional magnetic resonance imaging, HIFU, high-intensity focused ultrasound, ML, machine learning, MRGFUS, MR-guided focused ultrasound, MRI, magnetic resonance imaging, NMR, nuclear magnetic resonance, PACS, picture archiving and communication systems, PAMA, Protecting Access to Medicare Act, PET, positron emission tomography, PET-CT, positron emission tomography-computed tomography, PET-MRI, positron emission tomography magnetic resonance imaging, POCUS, point of care US, SPECT, single-photon emission computed tomography, US, ultrasound Machine learning, medical imaging, radiology, radiomics, theranostics},
  publisher    = {Rambam Health Corporation},
}

@Article{Beek2008,
  author       = {Edwin {J.R.} van Beek and Eric {A.} Hoffman},
  date         = {2009-03-01},
  journaltitle = {Clinics in Chest Medicine},
  title        = {Functional Imaging: {CT} and {MRI}},
  doi          = {10.1016/j.ccm.2007.12.003},
  number       = {1},
  pages        = {195--216},
  volume       = {29},
  abstract     = {Synopsis-Numerous imaging techniques permit evaluation of regional pulmonary function. Contrast-enhanced CT methods now allow assessment of vasculature and lung perfusion. Techniques using spirometric controlled MDCT allow for quantification of presence and distribution of parenchymal and airway pathology, Xenon gas can be employed to assess regional ventilation of the lungs and rapid bolus injections of iodinated contrast agent can provide quantitative measure of regional parenchymal perfusion. Advances in magnetic resonance imaging (MRI) of the lung include gadolinium-enhanced perfusion imaging and hyperpolarized helium imaging, which can allow imaging of pulmonary ventilation and .measurement of the size of emphysematous spaces.},
  day          = {1},
  file         = {:Segmentation/Beek2008 - Functional Imaging_ CT and MRI.pdf:PDF},
  groups       = {Segmentation},
  keywords     = {Quantitative CT, Airways, Parenchyma, Ventilation, Perfusion, Xenon CT, Computer Aided Detection, Lung Function, MDCT, MRI, Dual Source CT},
  month        = {3},
  publisher    = {Elsevier {BV}},
  year         = {2008},
}

@Article{Pandit2014,
  author       = {Prachi Pandit and Samuel M. Johnston and Yi Qi and Jennifer Story and Rendon Nelson and G. Allan Johnson},
  date         = {2014},
  journaltitle = {Academic Radiology},
  title        = {The Utility of Micro-{CT} and {MRI} in the Assessment of Longitudinal Growth of Liver Metastases in a Preclinical Model of Colon Carcinoma},
  doi          = {10.1016/j.acra.2012.09.030},
  number       = {4},
  volume       = {20},
  abstract     = {Rationale and Objectives-Liver is a common site for distal metastases in colon and rectal cancer. Numerous clinical studies have analyzed the relative merits of different imaging modalities for detection of liver metastases. A number of exciting new therapies are being investigated in preclinical models. But, technical challenges in preclinical imaging make it difficult to translate conclusions from clinical studies to the preclinical environment. This study addresses the technical challenges of preclinical MR and micro-CT to enable comparison of stateof-the-art methods for following metastatic liver disease. Materials and Methods-We optimized two promising preclinical protocols to enable a parallel longitudinal study tracking metastatic human colon carcinoma growth in a mouse model: T 2-weighted MRI using 2-shot PROPELLER (Periodically Rotated Overlapping ParallEL Lines with Enhanced Reconstruction), and contrast-enhanced micro-CT using a liposomal contrast agent. Both methods were tailored for high throughput with attention to animal support and anesthesia to limit biological stress. Results and Conclusions-Each modality has its strengths. Micro-CT permitted more rapid acquisition (<10 minutes) with the highest spatial resolution (88-micron isotropic resolution). But},
  file         = {:Segmentation/Pandit2014 - The Utility of Micro CT and MRI in the Assessment of Longitudinal Growth of Liver Metastases in a Preclinical Model of Colon Carcinoma.pdf:PDF},
  groups       = {Segmentation},
  keywords     = {PROPELLER, multi-modality, MR, CT, mice, liver metastases},
  publisher    = {Elsevier {BV}},
  year         = {2014},
}

@Book{Goodfellow2016,
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  date      = {2016},
  title     = {Deep Learning Book},
  publisher = {MIT Press},
  url       = {https://www.deeplearningbook.org/},
  groups    = {DL},
}

@Article{Litjens2017,
  author       = {Geert Litjens and Thijs Kooi and Babak Ehteshami Bejnordi and Arnaud Arindra Adiyoso Setio and Francesco Ciompi and Mohsen Ghafoorian and Jeroen A. W. M. van der Laak and Bram van Ginneken and Clara I. Sánchez},
  date         = {2017},
  journaltitle = {Medical Image Analysis},
  title        = {A survey on deep learning in medical image analysis},
  doi          = {10.1016/j.media.2017.07.005},
  issn         = {1361-8415},
  volume       = {42},
  file         = {:Segmentation/Litjens2017 - A Survey on Deep Learning in Medical Image Analysis.pdf:PDF},
  groups       = {Segmentation},
}

@Article{Shinohara2014,
  author       = {Russell T. Shinohara and Elizabeth M. Sweeney and Jeff Goldsmith and Navid Shiee and Farrah J. Mateen and Peter A. Calabresi and Samson Jarso and Dzung L. Pham and Daniel S. Reich and Ciprian M. Crainiceanu},
  date         = {2014-08-15},
  journaltitle = {{NeuroImage}: Clinical},
  title        = {Statistical normalization techniques for magnetic resonance imaging},
  doi          = {10.1016/j.nicl.2014.08.008},
  pages        = {9--19},
  volume       = {6},
  day          = {15},
  file         = {:Segmentation/Shinohara2014 - Statistical Normalization Techniques for Magnetic Resonance Imaging.pdf:PDF},
  groups       = {Segmentation},
  keywords     = {Magnetic resonance imaging Normalization Statistics Image analysis While computed tomography and other imaging techniques are measured in absolute units with physical meaning, magnetic resonance images are expressed in arbitrary units that are difficult to interpret and dif- fer between study visits and subjects. Much work in the image processing literature on intensity normal- ization has focused on histogram matching and other histogram mapping techniques, with little emphasis on normalizing images to have biologically interpretable units. Furthermore, there are no formalized prin- ciples or goals for the crucial comparability of image intensities within and across subjects. To address this, we propose a set of criteria necessary for the normalization of images. We further propose simple and ro- bust biologically motivated normalization techniques for multisequence brain imaging that have the same interpretation across acquisitions and satisfy the proposed criteria. We compare the performance of differ- ent normalization methods in thousands of images of patients with Alzheimer's disease, hundreds of ing for this project were funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant n Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the follow- oundation; BioClinica, Inc.; Biogen Idec Inc.; Bristol-Myers Squibb Company; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and company Genentech, Inc.; GE Healthcare; Innogenetics, N.V.; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Research & Development LLC.; Medpace, Inc.; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research; Novartis ; Servier; Synarc Inc.; and Takeda Pharmaceutical Company. The Canadian Institutes of Health Research is providing funds to ributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is ion, and the study is coordinated by the Alzheimer's Disease Cooperative Study at the University of California, Rev October 16, oratory for Neuro Imaging at the University of California, Los Angeles. This research was also supported by NIH grants P},
  month        = {8},
  publisher    = {Elsevier {BV}},
  year         = {2014},
}

@Article{Rorden2012,
  author       = {Christopher Rorden and Leonardo Bonilha and Julius Fridriksson and Benjamin Bender and Hans-Otto Karnath},
  date         = {2012},
  journaltitle = {{NeuroImage}},
  title        = {Age-specific {CT} and {MRI} templates for spatial normalization},
  doi          = {10.1016/j.neuroimage.2012.03.020},
  number       = {4},
  volume       = {61},
  abstract     = {Spatial normalization reshapes an individual's brain to match the shape and size of a template image. This is a crucial step required for group-level statistical analyses. The most popular standard templates are derived from MRI scans of young adults. We introduce specialized templates that allow normalization algorithms to be applied to stroke-aged populations. First, we developed a CT template: while this is the dominant modality for many clinical situations, there are no modern CT templates and popular algorithms fail to successfully normalize CT scans. Importantly, our template was based on healthy individuals with ages similar to what is commonly seen in stroke (mean 65 years old). This template allows studies where only CT scans are available. Second, we derived a MRI template that approximately matches the shape of our CT template as well as processing steps that aid the normalization of scans from older individuals (including lesion masking and the ability to generate high quality cortical renderings despite brain injury). The benefit of this strategy is that the resulting templates can be used in studies where mixed modalities are present. We have integrated these templates and processing algorithms into a simple SPM toolbox (http://www.mccauslandcenter.sc.edu/CRNL/tools/spm8-scripts).},
  file         = {:Segmentation/Rorden2012 - Age Specific CT and MRI Templates for Spatial Normalization.pdf:PDF},
  groups       = {Segmentation},
  keywords     = {voxel-based lesion symptom mapping, MRI, CT, stroke, aging, human},
  publisher    = {Elsevier {BV}},
}

@Article{Reppin2021,
  author       = {J. Reppin and C. Beyer and T. Hartmann and F. Schluenzen and M. Flemming and S. Sternberger and Y. Kemp},
  date         = {2021-06-09},
  journaltitle = {Computing and Software for Big Science},
  title        = {Interactive analysis notebooks on {DESY} batch resources},
  doi          = {10.1007/s41781-021-00058-y},
  number       = {1},
  volume       = {5},
  abstract     = {Batch scheduling systems are usually designed to maximise fair resource utilisation and efficiency, but are less well designed for demanding interactive processing, which requires fast access to resources while low upstart latency is only of secondary significance for high throughput of high performance computing scheduling systems. The computing clusters at DESY are intended as batch systems for end users to run massive analysis and simulation jobs enabling fast turnaround systems, in particular when processing is expected to feed back to operation of instruments in near real-time. The continuously increasing popularity of Jupyter Notebooks for interactive and online processing made an integration of this technology into the DESY batch systems indispensable. We present here our approach to utilise the HTCondor and SLURM backends to integrate Jupyter Notebook servers and the techniques involved to provide fast access. The chosen approach offers a smooth user experience allowing users to customize resource allocation tailored to their computational requirements. In addition, we outline the differences between the HPC and the HTC implementations and give an overview of the experience of running Jupyter Notebook services.},
  day          = {9},
  file         = {:DESY/Reppin2021 - Interactive Analysis Notebooks on DESY Batch Resources.pdf:PDF},
  groups       = {DESY},
  keywords     = {Jupyter, Notebooks, Interactive analysis, HTCondor, SLURM, Batch system},
  month        = {6},
  publisher    = {Springer Science and Business Media {LLC}},
  year         = {2021},
}

@WWW{FabianIsensee2020,
  author  = {FabianIsensee},
  date    = {2020-10-21},
  editor  = {Github},
  title   = {nnU-Net on Github},
  url     = {https://github.com/MIC-DKFZ/nnUNet},
  urldate = {2022-06-02},
  groups  = {CNN},
}

@Article{Baltruschat2021b,
  author       = {Baltruschat, Ivo and Ćwieka, Hanna and Krüger, Diana and Zeller-Plumhoff, Berit and Schl Ünzen, Frank and Willumeit-R Ömer, Regine and Moosmann, Julian and Heuser, Philipp},
  date         = {2021},
  journaltitle = {Scientific Reports},
  title        = {Scaling the U-net: Segmentation of biodegradable bone implants in high-resolution synchrotron radiation microtomograms - Supplementary Material},
  file         = {:CNN/Baltruschat2021b - Scaling the U Net_ Segmentation of Biodegradable Bone Implants in High Resolution Synchrotron Radiation Microtomograms Supplementary Material.pdf:PDF},
  groups       = {CNN},
}

@Article{Korteling2021,
  author       = {J. E. (Hans). Korteling and G. C. van de Boer-Visschedijk and R. A. M. Blankendaal and R. C. Boonekamp and A. R. Eikelboom},
  date         = {2021-03-25},
  journaltitle = {Frontiers in Artificial Intelligence},
  title        = {Human- versus Artificial Intelligence},
  doi          = {10.3389/frai.2021.622364},
  editor       = {Cesar Collazos},
  volume       = {4},
  abstract     = {AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions and, for instance, the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. In order to provide more agreement and to substantiate possible future research objectives, this paper presents three notions on the similarities and differences between human-and artificial intelligence: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple (integrated) forms of narrow-hybrid AI applications. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and "collaborate" with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the specific strengths of human-and artificial intelligence? How to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition (and vice versa)? Should we pursue the development of AI "partners" with human (-level) intelligence or should we focus more at supplementing human limitations? In order to answer these questions, humans working with AI systems in the workplace or in policy making have to develop an adequate mental model of the underlying 'psychological' mechanisms of AI. So, in order to obtain well-functioning human-AI systems, Intelligence Awareness in humans should be addressed more vigorously. For this purpose a first framework for educational content is proposed.},
  day          = {25},
  file         = {:ML&AI/Korteling2021 - Human Versus Artificial Intelligence.pdf:PDF},
  groups       = {ML&AI},
  keywords     = {human intelligence, artificial intelligence, artificial general intelligence, human-level artificial intelligence, cognitive complexity, narrow artificial intelligence, human-AI collaboration, cognitive bias},
  month        = {3},
  publisher    = {Frontiers Media {SA}},
  year         = {2021},
}

@InProceedings{VanGansbeke2021,
  author    = {Wouter Van Gansbeke and Simon Vandenhende and Stamatios Georgoulis and Luc Van Gool},
  booktitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
  date      = {2021},
  title     = {Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals},
  doi       = {10.1109/iccv48922.2021.00990},
  publisher = {{IEEE}},
  abstract  = {Being able to learn dense semantic representations of images without supervision is an important problem in computer vision. However, despite its significance, this problem remains rather unexplored, with a few exceptions that considered unsupervised semantic segmentation on small-scale datasets with a narrow visual domain. In this paper, we make a first attempt to tackle the problem on datasets that have been traditionally utilized for the supervised case. To achieve this, we introduce a two-step framework that adopts a predetermined mid-level prior in a contrastive optimization objective to learn pixel embeddings. This marks a large deviation from existing works that relied on proxy tasks or end-to-end clustering. Additionally, we argue about the importance of having a prior that contains information about objects, or their parts, and discuss several possibilities to obtain such a prior in an unsupervised manner. Experimental evaluation shows that our method comes with key advantages over existing works. First, the learned pixel embeddings can be directly clustered in semantic groups using K-Means on PASCAL. Under the fully unsupervised setting, there is no precedent in solving the semantic segmentation task on such a challenging benchmark. Second, our representations can improve over strong baselines when transferred to new datasets, e.g. COCO and DAVIS. The code is available 1 .},
  file      = {:Unsupervised Segmentation/VanGansbeke2021 - Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals.pdf:PDF},
  groups    = {Unsupervised Segmentation},
  priority  = {prio3},
}

@InProceedings{Chebli2018,
  author    = {Chebli, Asma and Djebbar, Akila and Farida Marouani, Hayet},
  booktitle = {2018 International Conference on Applied Smart Systems (ICASS'2018) 24-25 November 2018, Médéa, ALGERIA Semi-Supervised Learning for Medical Application},
  date      = {2018},
  title     = {Semi-Supervised Learning for Medical Application: A Survey},
  publisher = {IEEE},
  abstract  = {Developing a competent and accurate Computer-Aided Diagnosis (CAD) system to assist medical experts in making diagnosis requires a substantial amount of labeled (diagnosed) samples, however collecting labeled data is very costly and challenging when it comes to expert's annotation. This task is considered as a burden , and is both very time consuming and expensive. The framework of Semi-Supervised Learning (SSL) approach addresses this problem by taking advantage of the abundant amount of accessible unlabeled(undiagnosed) data together with the few limited labeled data in order to train precise classifiers while requiring less human effort and time. This paper reviews different CAD systems using SSL for numerous tasks ;the methods used and results obtained are discussed and key findings are highlighted ;to conclude with a presented proposed approach for the development of a CAD system ; applying Semi-Supervised learning for the classification of cases in order to improve the performance of Case-Based Reasoning(CBR) system.},
  file      = {:Semi-Supervised Segmentation/Chebli2018 - Semi Supervised Learning for Medical Application_ a Survey.pdf:PDF},
  groups    = {Semi-Supervised Segmentation},
  keywords  = {Semi-Supervised Learning, Computer-Aided diagnosis, Labeled data, unlabeled data},
  priority  = {prio1},
}

@Software{SchedMD2021,
  author  = {SchedMD},
  date    = {2021},
  title   = {SLURM Workload Manager},
  url     = {https://slurm.schedmd.com/},
  version = {20.11.7},
  groups  = {Software},
}

@Software{Eustace2022,
  author  = {Sébastien Eustace},
  date    = {2022},
  title   = {Poetry Dependency Manager},
  url     = {https://github.com/python-poetry/poetry},
  version = {1.1.13},
  groups  = {Software},
}

@Software{ThinkParQ2022,
  author = {ThinkParQ},
  date   = {2022},
  title  = {BeeGFS Parallel Cluster File System},
  url    = {https://www.beegfs.io/c/},
  groups = {Software},
}

@Software{Anaconda2020,
  author  = {{Anaconda Software Distribution}},
  date    = {2020},
  editor  = {{Anaconda Inc.}},
  title   = {Conda},
  url     = {https://www.anaconda.com/},
  version = {4.13.0},
  groups  = {Software},
}

@Software{2022,
  date    = {2022},
  title   = {GitLab Community Edition},
  url     = {https://gitlab.com/gitlab-org/gitlab},
  version = {15.2.1},
  groups  = {Software},
}

@Software{TFS2021,
  author  = {{Thermo Fisher Scientific}},
  date    = {2021},
  title   = {Avizo-Amira},
  version = {2021.1},
  groups  = {Software},
}

@InProceedings{Wilde2016,
  author    = {Fabian Wilde and Malte Ogurreck and Imke Greving and Jörg U. Hammel and Felix Beckmann and Alexander Hipp and Lars Lottermoser and Igor Khokhriakov and Pavel Lytaev and Thomas Dose and Hilmar Burmester and Martin Müller and Andreas Schreyer},
  booktitle = {{AIP} Conference Proceedings},
  date      = {2016},
  title     = {Micro-{CT} at the imaging beamline P05 at {PETRA} {III}},
  doi       = {10.1063/1.4952858},
  publisher = {Author(s)},
  abstract  = {The Imaging Beamline (IBL) P05 is operated by the Helmholtz-Zentrum Geesthacht and located at the DESY storage ring PETRA III. IBL is dedicated to X-ray full field imaging and consists of two experimental end stations. A micro tomography end station equipped for spatial resolutions down to 1 µm and a nano tomography end station for spatial resolutions down to 100 nm. The micro tomography end station is in user operation since 2013 and offers imaging with absorption contrast, phase enhanced absorption contrast and phase contrast methods. We report here on the current status and developments of the micro tomography end station including technical descriptions and show examples of research performed at P05.},
  file      = {:DESY/Wilde2016 - Micro CT at the Imaging Beamline P05 at PETRA III.pdf:PDF},
  groups    = {DESY},
  issn      = {0094-243X},
}

@WWW{DESf,
  author   = {{Deutsches Elektronen-Synchrtron (DESY)}},
  date     = {ohne Datum},
  title    = {Forschung für die {Zukunft}},
  url      = {https://www.desy.de/ueber_desy/desy/forschung_fuer_die_zukunft/index_ger.html},
  language = {de},
  urldate  = {2022-03-18},
  groups   = {DESY},
}

@WWW{DESa,
  author   = {{Deutsches Elektronen-Synchrtron (DESY)}},
  date     = {ohne Datum},
  title    = {{DESY} im Überblick},
  url      = {https://www.desy.de/ueber_desy/desy/index_ger.html},
  language = {de},
  urldate  = {2022-03-18},
  groups   = {DESY},
}

@WWW{DESb,
  author   = {{Deutsches Elektronen-Synchrtron (DESY)}},
  date     = {ohne Datum},
  title    = {Die {Rolle} der {Gruppen} {IT}/{DV} bei {DESY}},
  url      = {https://it.desy.de/wir_ueber_uns/leitlinie/index_ger.html},
  language = {de},
  urldate  = {2022-03-18},
  groups   = {DESY},
}

@WWW{DESc,
  author   = {{Deutsches Elektronen-Synchrtron (DESY)}},
  date     = {ohne Datum},
  title    = {Information über die {IT}-{Fachgruppen}},
  url      = {https://it.desy.de/wir_ueber_uns/gruppenleitung/index_ger.html},
  language = {de},
  urldate  = {2022-03-18},
  groups   = {DESY},
}

@WWW{DESd,
  author  = {{Deutsches Elektronen-Synchrtron (DESY)}},
  date    = {ohne Datum},
  title   = {PETRA III},
  url     = {https://www.desy.de/forschung/anlagen__projekte/petra_iii/index_ger.html},
  urldate = {2022-05-02},
  groups  = {DESY},
}

@WWW{DESe,
  author = {{Deutsches Elektronen-Synchrtron (DESY)}},
  date   = {ohne Datum},
  title  = {PETRA III Beamlines},
  url    = {https://photon-science.desy.de/facilities/petra_iii/beamlines/index.html},
  groups = {DESY},
}

@WWW{DES,
  author  = {{Deutsches Elektronen-Synchrotron, DESY}},
  title   = {P05 The Imaging Beamline at PETRA III},
  url     = {https://photon-science.desy.de/facilities/petra_iii/beamlines/p05_imaging_beamline/index_eng.html},
  urldate = {2022-07-29},
  groups  = {DESY},
}

@Article{Chen2017,
  author       = {Min Chen and Xiaobo Shi and Yin Zhang and Di Wu and Mohsen Guizani},
  date         = {2017},
  journaltitle = {{IEEE} Transactions on Big Data},
  title        = {Deep Feature Learning for Medical Image Analysis with Convolutional Autoencoder Neural Network},
  doi          = {10.1109/tbdata.2017.2717439},
  number       = {4},
  pages        = {750--758},
  volume       = {7},
  abstract     = {At present, computed tomography (CT) is widely used to assist disease diagnosis. Especially, computer aided diagnosis (CAD) based on artificial intelligence (AI) recently exhibits its importance in intelligent healthcare. However, it is a great challenge to establish an adequate labeled dataset for CT analysis assistance, due to the privacy and security issues. Therefore, this paper proposes a convolutional autoencoder deep learning framework to support unsupervised image features learning for lung nodule through unlabeled data, which only needs a small amount of labeled data for efficient feature learning. Through comprehensive experiments, it shows that the proposed scheme is superior to other approaches, which effectively solves the intrinsic labor-intensive problem during artificial image labeling. Moreover, it verifies that the proposed convolutional autoencoder approach can be extended for similarity measurement of lung nodules images. Especially, the features extracted through unsupervised learning are also applicable in other related scenarios.},
  file         = {:Semi-Supervised Segmentation/Chen2017 - Deep Feature Learning for Medical Image Analysis with Convolutional Autoencoder Neural Network.pdf:PDF},
  groups       = {Semi-Supervised Segmentation},
  keywords     = {Convolutional autoencoder neural network, Lung nodule, Feature learning, Hand-craft feature, Unsupervised learning},
  publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Feng2017,
  author    = {Xinyang Feng and Jie Yang and Andrew F. Laine and Elsa D. Angelini},
  booktitle = {International Conference on Medical Image Computing and Computer-Assisted Intervention},
  date      = {2017},
  title     = {Discriminative Localization in CNNs for Weakly-Supervised Segmentation of Pulmonary Nodules},
  doi       = {10.1007/978-3-319-66179-7_65},
  pages     = {568-576},
  abstract  = {Automated detection and segmentation of pulmonary nodules on lung computed tomography (CT) scans can facilitate early lung cancer diagnosis. Existing supervised approaches for automated nodule segmentation on CT scans require voxel-based annotations for training, which are labor-and time-consuming to obtain. In this work, we propose a weakly-supervised method that generates accurate voxel-level nodule segmentation trained with image-level labels only. By adapting a convolutional neural network (CNN) trained for image classification, our proposed method learns discriminative regions from the activation maps of convolution units at different scales, and identifies the true nodule location with a novel candidate-screening framework. Experimental results on the public LIDC-IDRI dataset demonstrate that, our weaklysupervised nodule segmentation framework achieves competitive performance compared to a fully-supervised CNN-based segmentation method.},
  file      = {:Semi-Supervised Segmentation/Feng2017 - Discriminative Localization in CNNs for Weakly Supervised Segmentation of Pulmonary Nodules.pdf:PDF},
  groups    = {Semi-Supervised Segmentation},
  issn      = {0302-9743},
}

@Article{Gu2019,
  author     = {Zaiwang Gu and Jun Cheng and Huazhu Fu and Kang Zhou and Huaying Hao and Yitian Zhao and Tianyang Zhang and Shenghua Gao and Jiang Liu},
  date       = {2019},
  title      = {CE-Net: Context Encoder Network for 2D Medical Image Segmentation},
  doi        = {10.1109/tmi.2019.2903562},
  eprint     = {1903.02740},
  eprinttype = {arXiv},
  issn       = {0278-0062},
  pages      = {2281-2292},
  url        = {https://www.semanticscholar.org/paper/a07aa5b834e5083624189a929be07c7ae2389229},
  volume     = {38},
  abstract   = {Medical image segmentation is an important step in medical image analysis. With the rapid development of convolutional neural network in image processing, deep learning has been used for medical image segmentation, such as optic disc segmentation, blood vessel detection, lung segmentation, cell segmentation, etc. Previously, U-net based approaches have been proposed. However, the consecutive pooling and strided convolutional operations lead to the loss of some spatial information. In this paper, we propose a context encoder network (referred to as CE-Net) to capture more high-level information and preserve spatial information for 2D medical image segmentation. CE-Net mainly contains three major components: a feature encoder module, a context extractor and a feature decoder module. We use pretrained ResNet block as the fixed feature extractor. The context extractor module is formed by a newly proposed dense atrous convolution (DAC) block and residual multi-kernel pooling (RMP) block. We applied the proposed CE-Net to different 2D medical image segmentation tasks. Comprehensive results show that the proposed method outperforms the original U-Net method and other state-of-the-art methods for optic disc segmentation, vessel detection, lung segmentation, cell contour segmentation and retinal optical coherence tomography layer segmentation.},
  booktitle  = {IEEE Transactions on medical Imaging},
  file       = {:Unsupervised Segmentation/Gu2019 - CE Net_ Context Encoder Network for 2D Medical Image Segmentation.pdf:PDF},
  groups     = {Unsupervised Segmentation},
  keywords   = {Medical image segmentation, Deep Learning, Context encoder network I. INTRODUCTION Medical image segmentation is often an important step in medical image analysis, such as optic disc segmentation [1], [2], [3] and blood vessel detection [4], [5], [6], [7], [8] in retinal images, cell segmentation [9], [10], [11] in electron microscopic (EM) recordings, lung segmentation [12]},
  pmid       = {30843824},
  venue      = {IEEE Transactions on Medical Imaging},
}

@Article{Oktay2018,
  author       = {Ozan Oktay and Enzo Ferrante and Konstantinos Kamnitsas and Mattias Heinrich and Wenjia Bai and Jose Caballero and Stuart A. Cook and Antonio de Marvao and Timothy Dawes and Declan P. O{\textquotesingle}Regan and Bernhard Kainz and Ben Glocker and Daniel Rueckert},
  date         = {2018-02},
  journaltitle = {{IEEE} Transactions on Medical Imaging},
  title        = {Anatomically Constrained Neural Networks ({ACNNs}): Application to Cardiac Image Enhancement and Segmentation},
  doi          = {10.1109/tmi.2017.2743464},
  pages        = {384--395},
  volume       = {37},
  file         = {:Semi-Supervised Segmentation/Oktay2018 - Anatomically Constrained Neural Networks (ACNNs)_ Application to Cardiac Image Enhancement and Segmentation.pdf:PDF},
  groups       = {Semi-Supervised Segmentation},
  publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{Kanezaki2018,
  author    = {Kanezaki, Asako},
  booktitle = {{IEEE} International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  date      = {2018},
  title     = {UNSUPERVISED IMAGE SEGMENTATION BY BACKPROPAGATION},
  doi       = {10.1109/ICASSP.2018.8462533},
  publisher = {{IEEE}},
  abstract  = {We investigate the use of convolutional neural networks (CNNs) for unsupervised image segmentation. As in the case of supervised image segmentation, the proposed CNN assigns labels to pixels that denote the cluster to which the pixel belongs. In the unsupervised scenario, however, no training images or ground truth labels of pixels are given beforehand. Therefore, once when a target image is input, we jointly optimize the pixel labels together with feature representations while their parameters are updated by gradient descent. In the proposed approach, we alternately iterate label prediction and network parameter learning to meet the following criteria: (a) pixels of similar features are desired to be assigned the same label, (b) spatially continuous pixels are desired to be assigned the same label, and (c) the number of unique labels is desired to be large. Although these criteria are incompatible, the proposed approach finds a plausible solution of label assignment that balances well the above criteria, which demonstrates good performance on a benchmark dataset of image segmentation.},
  file      = {:Unsupervised Segmentation/Kanezaki2018 - UNSUPERVISED IMAGE SEGMENTATION bY BACKPROPAGATION.pdf:PDF},
  groups    = {Unsupervised Segmentation},
  keywords  = {Convolutional neural networks, Unsupervised learning, Feature clustering},
  priority  = {prio2},
}

@Article{Aganj2018,
  author       = {Iman Aganj and Mukesh G. Harisinghani and Ralph Weissleder and Bruce Fischl},
  date         = {2018-08},
  journaltitle = {Scientific Reports},
  title        = {Unsupervised Medical Image Segmentation Based on the Local Center of Mass},
  doi          = {10.1038/s41598-018-31333-5},
  number       = {1},
  volume       = {8},
  abstract     = {Image segmentation is a critical step in numerous medical imaging studies, which can be facilitated by automatic computational techniques. Supervised methods, although highly effective, require large training datasets of manually labeled images that are labor-intensive to produce. Unsupervised methods, on the contrary, can be used in the absence of training data to segment new images. We introduce a new approach to unsupervised image segmentation that is based on the computation of the local center of mass. We propose an efficient method to group the pixels of a one-dimensional signal, which we then use in an iterative algorithm for two-and three-dimensional image segmentation. We validate our method on a 2D X-ray image, a 3D abdominal magnetic resonance (MR) image and a dataset of 3D cardiovascular MR images.},
  file         = {:Unsupervised Segmentation/Aganj2018 - Unsupervised Medical Image Segmentation Based on the Local Center of Mass.pdf:PDF},
  groups       = {Unsupervised Segmentation},
  priority     = {prio2},
  publisher    = {Springer Science and Business Media {LLC}},
}

@Article{Xia2017,
  author     = {Xia, Xide and Kulis, Brian},
  date       = {2017},
  title      = {W-Net: A Deep Model for Fully Unsupervised Image Segmentation},
  doi        = {10.48550/ARXIV.1711.08506},
  eprint     = {1711.08506},
  eprinttype = {arXiv},
  url        = {https://www.semanticscholar.org/paper/f8a727877dd258f80eec6d075a3d440c2ac98d36},
  abstract   = {While significant attention has been recently focused on designing supervised deep semantic segmentation algorithms for vision tasks, there are many domains in which sufficient supervised pixel-level labels are difficult to obtain. In this paper, we revisit the problem of purely unsupervised image segmentation and propose a novel deep architecture for this problem. We borrow recent ideas from supervised semantic segmentation methods, in particular by concatenating two fully convolutional networks together into an autoencoder--one for encoding and one for decoding. The encoding layer produces a k-way pixelwise prediction, and both the reconstruction error of the autoencoder as well as the normalized cut produced by the encoder are jointly minimized during training. When combined with suitable postprocessing involving conditional random field smoothing and hierarchical segmentation, our resulting algorithm achieves impressive results on the benchmark Berkeley Segmentation Data Set, outperforming a number of competing methods.},
  copyright  = {arXiv.org perpetual, non-exclusive license},
  file       = {:Unsupervised Segmentation/Xia2017 - W Net_ a Deep Model for Fully Unsupervised Image Segmentation.pdf:PDF},
  groups     = {Unsupervised Segmentation},
  keywords   = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences},
  priority   = {prio2},
  publisher  = {arXiv},
  venue      = {ArXiv},
}

@Article{Ejaz2019,
  author    = {Khurram Ejaz and Mohd Shafy Mohd Rahim and Usama Ijaz Bajwa and Nadim Rana and Amjad Rehman},
  date      = {2019},
  title     = {An Unsupervised Learning with Feature Approach for Brain Tumor Segmentation Using Magnetic Resonance Imaging},
  doi       = {10.1145/3314367.3314384},
  url       = {https://www.semanticscholar.org/paper/47616b9018f45722ba1d3b23d784a96d1ab62825},
  abstract  = {Segmentation methods are so much efficient to segment complex tumor from challenging datasets. MACCAI BRATS 2013-2017 brain tumor dataset (FLAIR, T2) had been taken for high grade glioma (HGG). This data set is challenging to segment tumor due to homogenous intensity and difficult to separate tumor boundary from other normal tissues, so our goal is to segment tumor from mixed intensities. It can be accomplished step by step. Therefore image maximum and minimum intensities has been adjusted because need to highlight the tumor portion then thresholding perform to localize the tumor region, has applied statistical features(kurtosis, skewness, mean and variance) so tumor portion become more visualize but cann't separate tumor from boundary and then apply unsupervised clusters like kmean but it gives hard crisp membership and many tumor membership missed so texture features(Correlation, energy, homogeneity and contrast) with combination of Gabor filter has been applied but dimension of data increase and intensities became disturb due high dimension operation over MRI. Tumor boundary become more visualize if combine FLAIR over T2 sequence image then we apply FCM and result is: tumor boundaries become more visualized then applied one statistical feature (Kurtosis) and one texture feature(Energy) so tumor portion separate from other tissue and better segmentation accuracy have been checked with comparison parameters like dice overlap and Jaccard index.},
  booktitle = {Proceedings of the 2019 9th International Conference on Bioscience, Biochemistry and Bioinformatics - {ICBBB} {\textquotesingle}19},
  file      = {:Unsupervised Segmentation/Ejaz2019 - An Unsupervised Learning with Feature Approach for Brain Tumor Segmentation Using Magnetic Resonance Imaging.pdf:PDF},
  groups    = {Unsupervised Segmentation},
  keywords  = {CCS Concepts SOM-FKM, Magnetic Resonance Imaging (MRI), Texture features, statistical features, Kmean, FCM. Dice index (DOI), Jaccard Index(JI)},
  publisher = {{ACM} Press},
  venue     = {ICBBB '19},
}

@Article{Raza2018,
  author       = {Khalid Raza and Nripendra Kumar Singh},
  journaltitle = {Current Medical Imaging Formerly Current Medical Imaging Reviews},
  title        = {A Tour of Unsupervised Deep Learning for Medical Image Analysis},
  doi          = {10.2174/1573405617666210127154257},
  issn         = {1573-4056},
  number       = {9},
  pages        = {1059--1077},
  volume       = {17},
  abstract     = {Interpretation of medical images for diagnosis and treatment of complex disease from highdimensional and heterogeneous data remains a key challenge in transforming healthcare. In the last few years, both supervised and unsupervised deep learning achieved promising results in the area of medical imaging and image analysis. Unlike supervised learning which is biased towards how it is being supervised and manual efforts to create class label for the algorithm, unsupervised learning derive insights directly from the data itself, group the data and help to make data driven decisions without any external bias. This review systematically presents various unsupervised models applied to medical image analysis, including autoencoders and its several variants, Restricted Boltzmann machines, Deep belief networks, Deep Boltzmann machine and Generative adversarial network. Future research opportunities and challenges of unsupervised techniques for medical image analysis have also been discussed.},
  day          = {13},
  file         = {:Unsupervised Segmentation/Raza2018 - A Tour of Unsupervised Deep Learning for Medical Image Analysis.pdf:PDF},
  groups       = {Unsupervised Segmentation},
  keywords     = {Unsupervised learning, medical image analysis, autoencoders, restricted Boltzmann machine, Deep belief network},
  month        = {12},
  priority     = {prio1},
  publisher    = {Bentham Science Publishers Ltd.},
  year         = {2018},
}

@Article{Yoo2014,
  author    = {Y. Yoo and T. Brosch and A. Traboulsee and David K.B. Li and R. Tam},
  date      = {2014},
  title     = {Deep Learning of Image Features from Unlabeled Data for Multiple Sclerosis Lesion Segmentation},
  doi       = {10.1007/978-3-319-10581-9_15},
  editor    = {G. Wu and others},
  issn      = {0302-9743},
  pages     = {117-124},
  series    = {LNCS},
  url       = {https://www.semanticscholar.org/paper/caf2e76725224cecd348da8236ac508ccb15bcd2},
  volume    = {8679},
  abstract  = {A new automatic method for multiple sclerosis (MS) lesion segmentation in multi-channel 3D MR images is presented. The main novelty of the method is that it learns the spatial image features needed for training a supervised classifier entirely from unlabeled data. This is in contrast to other current supervised methods, which typically require the user to preselect or design the features to be used. Our method can learn an extensive set of image features with minimal user effort and bias. In addition, by separating the feature learning from the classifier training that uses labeled (pre-segmented data), the feature learning can take advantage of the typically much more available unlabeled data. Our method uses deep learning for feature learning and a random forest for supervised classification, but potentially any supervised classifier can be used. Quantitative validation is carried out using 1450 T2-weighted and PD-weighted pairs of MRIs of MS patients, with 1400 pairs used for feature learning (100 of those for labeled training), and 50 for testing. The results demonstrate that the learned features are highly competitive with hand-crafted features in terms of segmentation accuracy, and that segmentation performance increases with the amount of unlabeled data used, even when the number of labeled images is fixed.},
  booktitle = {MLMI 2014},
  file      = {:Unsupervised Segmentation/Yoo2014 - Deep Learning of Image Features from Unlabeled Data for Multiple Sclerosis Lesion Segmentation.pdf:PDF},
  groups    = {Unsupervised Segmentation},
  keywords  = {Multiple sclerosis lesions, MRI, machine learning, segmentation, deep learning, random forests},
  publisher = {Springer},
  venue     = {MLMI},
}

@InProceedings{Bi2018,
  author   = {Bi, Lei and Feng, Dagan and Kim, Jinman},
  date     = {2018-04-30},
  title    = {Dual-Path Adversarial Learning for Fully Convolutional Network (FCN)-Based Medical Image Segmentation},
  doi      = {10.1007/s00371-018-1519-5},
  abstract = {Segmentation of regions of interest (ROIs) in medical images is an important step for image analysis in computer-aided diagnosis systems. In recent years, segmentation methods based on fully convolutional networks (FCNs) have achieved great success in general images. FCN performance is primarily due to it leveraging large labeled datasets to hierarchically learn the features that correspond to the shallow appearance as well as the deep semantics of the images. However, such dependence on large dataset does not translate well into medical images where there is a scarcity of annotated medical training data, and FCN results in coarse ROI detections and poor boundary definitions. To overcome this limitation, medical image-specific FCN methods have been introduced with post-processing techniques to refine the segmentation results; however, the performance of these methods is reliant on the appropriate tuning of a large number of parameters and dependence on data-specific postprocessing techniques. In this study, we leverage the state-of-the-art image feature learning method of generative adversarial network (GAN) for its inherent ability to produce consistent and realistic images features by using deep neural networks and adversarial learning concept. We improve upon GAN such that ROI features can be learned at different levels of complexities (simple and complex), in a controlled manner, via our proposed dual-path adversarial learning (DAL). The outputs from our DAL are then augmented to the learned ROI features into the existing FCN training data, which increases the overall feature diversity. We conducted experiments on three public datasets with a variety of visual characteristics. Our results demonstrate that our DAL can improve FCN-based segmentation methods and outperform or be competitive in performances to the state-of-the-art methods without using medical image-specific optimizations.},
  day      = {30},
  file     = {:Semi-Supervised Segmentation/Bi2018 - Dual Path Adversarial Learning for Fully Convolutional Network (FCN) Based Medical Image Segmentation.pdf:PDF},
  groups   = {Semi-Supervised Segmentation},
  keywords = {Adversarial learning, Fully convolutional networks (FCNs), Segmentation, Regions of interest (ROI)},
  month    = {4},
  year     = {2018},
}

@Article{Carneiro2013,
  author       = {Carneiro, Gustavo and Nascimento, Jacinto C.},
  date         = {2013},
  journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title        = {Combining multiple dynamic models and deep learning architectures for tracking the left ventricle endocardium in ultrasound data},
  file         = {:Unsupervised Segmentation/Carneiro2013 - Combining Multiple Dynamic Models and Deep Learning Architectures for Tracking the Left Ventricle Endocardium in Ultrasound Data.pdf:PDF},
  groups       = {Unsupervised Segmentation},
  publisher    = {IEEE},
}

@Article{Carneiro2012,
  author       = {Gustavo Carneiro and Jacinto C. Nascimento and António Freitas},
  date         = {2012},
  journaltitle = {IEEE TRANSACTIONS ON IMAGE PROCESSING},
  title        = {The Segmentation of the Left Ventricle of the Heart From Ultrasound Data Using Deep Learning Architectures and Derivative-Based Search Methods},
  file         = {:Unsupervised Segmentation/Carneiro2012 - The Segmentation of the Left Ventricle of the Heart from Ultrasound Data Using Deep Learning Architectures and Derivative Based Search Methods.pdf:PDF},
  groups       = {Unsupervised Segmentation},
}

@InCollection{Su2018,
  author    = {Hai Su and Fuyong Xing and Xiangfei Kong and Yuanpu Xie and Shaoting Zhang and Lin Yang},
  booktitle = {Deep Learning and Convolutional Neural Networks for Medical Image Computing},
  date      = {2018},
  title     = {Robust Cell Detection and Segmentation in Histopathological Images Using Sparse Reconstruction and Stacked Denoising Autoencoders},
  doi       = {10.1007/978-3-319-42999-1_15},
  pages     = {257--278},
  publisher = {Springer International Publishing},
  url       = {https://www.semanticscholar.org/paper/a3fe9f3b248417db3cdcf07ab6f9a63c03a6345f},
  abstract  = {Computer-aided diagnosis (CAD) is a promising tool for accurate and consistent diagnosis and prognosis. Cell detection and segmentation are essential steps for CAD. These tasks are challenging due to variations in cell shapes, touching cells, and cluttered background. In this paper, we present a cell detection and segmentation algorithm using the sparse reconstruction with trivial templates and a stacked denoising autoencoder (sDAE). The sparse reconstruction handles the shape variations by representing a testing patch as a linear combination of shapes in the learned dictionary. Trivial templates are used to model the touching parts. The sDAE, trained with the original data and their structured labels, is used for cell segmentation. To the best of our knowledge, this is the first study to apply sparse reconstruction and sDAE with structured labels for cell detection and segmentation. The proposed method is extensively tested on two data sets containing more than 3000 cells obtained from brain tumor and lung cancer images. Our algorithm achieves the best performance compared with other state of the arts.},
  file      = {:Unsupervised Segmentation/Su2018 - Robust Cell Detection and Segmentation in Histopathological Images Using Sparse Reconstruction and Stacked Denoising Autoencoders.pdf:PDF},
  groups    = {Unsupervised Segmentation},
  pmid      = {27796013},
}

@Article{Mansoor2016,
  author    = {Awais Mansoor and Juan J. Cerrolaza and Rabia Idrees and E. Biggs and M. Alsharid and R. Avery and M. Linguraru},
  date      = {2016},
  title     = {Deep Learning Guided Partitioned Shape Model for Anterior Visual Pathway Segmentation},
  doi       = {10.1109/TMI.2016.2535222},
  url       = {https://www.semanticscholar.org/paper/d458bc3a2f68bd2594a4d77b341082ffa59114f4},
  abstract  = {Analysis of cranial nerve systems, such as the anterior visual pathway (AVP), from MRI sequences is challenging due to their thin long architecture, structural variations along the path, and low contrast with adjacent anatomic structures. Segmentation of a pathologic AVP (e.g., with low-grade gliomas) poses additional challenges. In this work, we propose a fully automated partitioned shape model segmentation mechanism for AVP steered by multiple MRI sequences and deep learning features. Employing deep learning feature representation, this framework presents a joint partitioned statistical shape model able to deal with healthy and pathological AVP. The deep learning assistance is particularly useful in the poor contrast regions, such as optic tracts and pathological areas. Our main contributions are: 1) a fast and robust shape localization method using conditional space deep learning, 2) a volumetric multiscale curvelet transform-based intensity normalization method for robust statistical model, and 3) optimally partitioned statistical shape and appearance models based on regional shape variations for greater local flexibility. Our method was evaluated on MRI sequences obtained from 165 pediatric subjects. A mean Dice similarity coefficient of 0.779 was obtained for the segmentation of the entire AVP (optic nerve only =0.791) using the leave-one-out validation. Results demonstrated that the proposed localized shape and sparse appearance-based learning approach significantly outperforms current state-of-the-art segmentation approaches and is as robust as the manual segmentation.},
  file      = {:Unsupervised Segmentation/Mansoor2016 - Deep Learning Guided Partitioned Shape Model for Anterior Visual Pathway Segmentation.pdf:PDF},
  groups    = {Unsupervised Segmentation},
  keywords  = {Anterior visual pathway, intensity normalization, MRI, partitioned statistical model, shape model, sparse learning},
  pmid      = {26930677},
  publisher = {IEEE},
  venue     = {IEEE Transactions on Medical Imaging},
}

@Article{Ngo2016,
  author       = {Ngo, Tuan and Lu, Zhi and Carneiro, Gustavo},
  date         = {2017},
  journaltitle = {Med. Image Anal.},
  title        = {Combining deep learning and level set for the automated segmentation of the left ventricle of the heart from cardiac cine magnetic resonance},
  doi          = {10.1016/j.media.2016.05.009},
  url          = {https://www.semanticscholar.org/paper/c60d351775053356dba871402fac2b180a5a18b3},
  abstract     = {We introduce a new methodology that combines deep learning and level set for the automated segmentation of the left ventricle of the heart from cardiac cine magnetic resonance (MR) data. This combination is relevant for segmentation problems, where the visual object of interest presents large shape and},
  day          = {20},
  file         = {:Unsupervised Segmentation/Ngo2016 - Combining Deep Learning and Level Set for the Automated Segmentation of the Left Ventricle of the Heart from Cardiac Cine Magnetic Resonance.pdf:PDF},
  groups       = {Unsupervised Segmentation},
  keywords     = {Deep learning, Level set method, Segmentation of the Left Ventricle of the Heart, Cardiac Cine Magnetic Resonance},
  month        = {5},
  pmid         = {27423113},
  venue        = {Medical Image Anal.},
  year         = {2016},
}

@Article{Pereira2018,
  author       = {S{\'{e}}rgio Pereira and Raphael Meier and Richard McKinley and Roland Wiest and Victor Alves and Carlos A. Silva and Mauricio Reyes},
  journaltitle = {Medical Image Analysis},
  title        = {Enhancing interpretability of automatically extracted machine learning features: application to a {RBM}-Random Forest system on brain lesion segmentation},
  doi          = {10.1016/j.media.2017.12.009},
  pages        = {228--244},
  volume       = {44},
  abstract     = {Machine learning systems are achieving better performances at the cost of becoming increasingly complex. However, because of that, they become less interpretable, which may cause some distrust by the end-user of the system. This is especially important as these systems are pervasively being introduced to critical domains, such as the medical field. Representation Learning techniques are general methods for automatic feature computation. Nevertheless, these techniques are regarded as uninterpretable "black boxes". In this paper, we propose a methodology to enhance the interpretability of automatically extracted machine learning features. The proposed system is composed of a Restricted Boltzmann Machine for unsupervised feature learning, and a Random Forest classifier, which are combined to jointly consider existing correlations between imaging data, features, and target variables. We define two levels of interpretation: global and local. The former is devoted to understanding if the system learned the relevant relations in the data correctly, while the later is focused on predictions performed on a voxel-and patient-level. In addition, we propose a novel feature importance strategy that considers both imaging data and target variables, and we demonstrate the ability of the approach to leverage the interpretability of the obtained representation for the task at hand. We evaluated the proposed methodology in brain tumor segmentation and penumbra estimation in ischemic stroke lesions. We show the ability of the proposed methodology to unveil information regarding relationships between imaging modalities and extracted features and their usefulness for the task at hand. In both clinical scenarios, we demonstrate that the proposed methodology enhances the interpretability of automatically learned features, highlighting specific learning patterns that resemble how an expert extracts relevant data from medical images.},
  day          = {20},
  file         = {:Unsupervised Segmentation/Pereira2018 - Enhancing Interpretability of Automatically Extracted Machine Learning Features_ Application to a RBM Random Forest System on Brain Lesion Segmentation.pdf:PDF},
  groups       = {Unsupervised Segmentation},
  keywords     = {Interpretability, Machine learning, Representation learning},
  month        = {2},
  publisher    = {Elsevier {BV}},
  year         = {2018},
}

@Article{Hou2019,
  author       = {Le Hou and Vu Nguyen and Ariel B. Kanevsky and Dimitris Samaras and Tahsin M. Kurc and Tianhao Zhao and Rajarsi R. Gupta and Yi Gao and Wenjin Chen and David Foran and Joel H. Saltz},
  date         = {2017},
  journaltitle = {Pattern Recognition},
  title        = {Sparse Autoencoder for Unsupervised Nucleus Detection and Representation in Histopathology Images},
  doi          = {10.1016/j.patcog.2018.09.007},
  eprint       = {1704.00406},
  eprinttype   = {arXiv},
  pages        = {188--200},
  url          = {https://www.semanticscholar.org/paper/ab5ae75976fcf0c31e6f537a9fdf60acde1aa2ff},
  volume       = {86},
  abstract     = {We propose a sparse Convolutional Autoencoder (CAE) for simultaneous nucleus detection and feature extraction in histopathology tissue images. Our CAE detects and encodes nuclei in image patches in tissue images into sparse feature maps that encode both the location and appearance of nuclei. A primary contribution of our work is the development of an unsupervised detection network by using the characteristics of histopathology image patches. The pretrained nucleus detection and feature extraction modules in our CAE can be fine-tuned for supervised learning in an end-to-end fashion. We evaluate our method on four datasets and achieve state-of-the-art results. In addition, we are able to achieve comparable performance with only 5% of the fullysupervised annotation cost. Publisher's Disclaimer: This is a PDF file of an unedited manuscript that has been accepted for publication. As a service to our customers we are providing this early version of the manuscript. The manuscript will undergo copyediting, typesetting, and review of the resulting proof before it is published in its final citable fsorm. Please note that during the production process errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.},
  day          = {1},
  file         = {:Unsupervised Segmentation/Hou2019 - Sparse Autoencoder for Unsupervised Nucleus Detection and Representation in Histopathology Images.pdf:PDF},
  groups       = {Unsupervised Segmentation},
  keywords     = {pathology image analysis, convolutional neural network, unsupervised learning, semi-supervised learning},
  month        = {2},
  pmid         = {30631215},
  publisher    = {Elsevier {BV}},
  venue        = {Pattern Recognit.},
  year         = {2019},
}

@Article{Guo2014,
  author    = {Guo, Yanrong and Wu, Guorong and Commander, Leah and Szary, Stephanie and Jewells, Valerie and Lin, Weili and Shen, Dinggang},
  date      = {2014},
  title     = {Segmenting Hippocampus from Infant Brains by Sparse Patch Matching with Deep-Learned Features},
  doi       = {10.1007/978-3-319-10470-6_39},
  url       = {https://www.semanticscholar.org/paper/573fd9ea9da232cc76cec3d13c7113b67125801e},
  abstract  = {Accurate segmentation of the hippocampus from infant MR brain images is a critical step for investigating early brain development. Unfortunately, the previous tools developed for adult hippocampus segmentation are not suitable for infant brain images acquired from the first year of life, which often have poor tissue contrast and variable structural patterns of early hippocampal development. From our point of view, the main problem is lack of discriminative and robust feature representations for distinguishing the hippocampus from the surrounding brain structures. Thus, instead of directly using the predefined features as popularly used in the conventional methods, we propose to learn the latent feature representations of infant MR brain images by unsupervised deep learning. Since deep learning paradigms can learn low-level features and then successfully build up more comprehensive high-level features in a layer-by-layer manner, such hierarchical feature representations can be more competitive for distinguishing the hippocampus from entire brain images. To this end, we apply Stacked Auto Encoder (SAE) to learn the deep feature representations from both T1-and T2-weighed MR images combining their complementary information, which is important for characterizing different development stages of infant brains after birth. Then, we present a sparse patch matching method for transferring hippocampus labels from multiple atlases to the new infant brain image, by using deep-learned feature representations to measure the inter-patch similarity. Experimental results on 2-week-old to 9-month-old infant brain images show the effectiveness of the proposed method, especially compared to the state-of-the-art counterpart methods.},
  booktitle = {Medical image computing and computer-assisted intervention : MICCAI},
  file      = {:Unsupervised Segmentation/Guo2014 - Segmenting Hippocampus from Infant Brains by Sparse Patch Matching with Deep Learned Features.pdf:PDF},
  groups    = {Unsupervised Segmentation},
  pmid      = {25485393},
  venue     = {MICCAI},
}

@Article{Hatipoglu2017,
  author       = {Nuh Hatipoglu and G. Bilgin},
  date         = {2017},
  journaltitle = {Medical {\&}amp$\mathsemicolon$ Biological Engineering {\&}amp$\mathsemicolon$ Computing},
  title        = {Cell segmentation in histopathological images with deep learning algorithms by utilizing spatial relationships},
  doi          = {10.1007/s11517-017-1630-1},
  number       = {10},
  pages        = {1829--1848},
  url          = {https://www.semanticscholar.org/paper/15d686173b8ab5db7e59888f30602c52868d3d2e},
  volume       = {55},
  abstract     = {Medical & Biological Engineering & Computing, doi:10.1007/s11517-017-1630-1},
  file         = {:Unsupervised Segmentation/Hatipoglu2017 - Cell Segmentation in Histopathological Images with Deep Learning Algorithms by Utilizing Spatial Relationships.pdf:PDF},
  groups       = {Unsupervised Segmentation},
  keywords     = {Histopathological images, Deep learning algorithms, Computer-aided diagnosis systems, Segmentation, Spatial relationships},
  pmid         = {28247185},
  publisher    = {Springer Science and Business Media {LLC}},
  venue        = {Medical & Biological Engineering & Computing},
}

@Article{Avendi2017,
  author       = {Michael R. Avendi and Arash Kheradvar and Hamid Jafarkhani},
  date         = {2017-02-16},
  journaltitle = {Magnetic Resonance in Medicine},
  title        = {Automatic segmentation of the right ventricle from cardiac {MRI} using a learning-based approach},
  doi          = {10.1002/mrm.26631},
  number       = {6},
  pages        = {2439--2448},
  volume       = {78},
  abstract     = {Purpose: This study aims to accurately segment the right ventricle (RV) from cardiac MRI using a fully automatic learningbased method. Methods: The proposed method uses deep learning algorithms, i.e., convolutional neural networks and stacked autoencoders, for automatic detection and initial segmentation of the RV chamber. The initial segmentation is then combined with the deformable models to improve the accuracy and robustness of the process. We trained our algorithm using 16 cardiac MRI datasets of the MICCAI 2012 RV Segmentation Challenge database and validated our technique using the rest of the dataset (32 subjects). Results: An average Dice metric of 82.5% along with an average Hausdorff distance of 7.85 mm were achieved for all the studied subjects. Furthermore, a high correlation and level of agreement with the ground truth contours for end-diastolic volume (0.98), end-systolic volume (0.99), and ejection fraction (0.93) were observed. Conclusion: Our results show that deep learning algorithms can be effectively used for automatic segmentation of the RV. Computed quantitative metrics of our method outperformed that of the existing techniques participated in the MICCAI 2012 challenge, as reported by the challenge organizers.},
  day          = {16},
  file         = {:Unsupervised Segmentation/Avendi2017 - Automatic Segmentation of the Right Ventricle from Cardiac MRI Using a Learning Based Approach.pdf:PDF},
  groups       = {Unsupervised Segmentation},
  keywords     = {cardiac MRI, right ventricle, segmentation, deep learning, deformable models},
  month        = {2},
  publisher    = {Wiley},
  year         = {2017},
}

@InProceedings{Chen2020,
  author   = {Chen, Junyu and Frey, Eric and Frey, Chen},
  date     = {2020-07-06},
  title    = {Medical Image Segmentation via Unsupervised Convolutional Neural Network},
  eprint   = {arXiv:2001.10155v4[cs.CV]},
  abstract = {For the majority of the learning-based segmentation methods, a large quantity of highquality training data is required. In this paper, we present a novel learning-based segmentation model that could be trained semi-or un-supervised. Specifically, in the unsupervised setting, we parameterize the Active contour without edges (ACWE) framework via a convolutional neural network (ConvNet), and optimize the parameters of the Con-vNet using a self-supervised method. In another setting (semi-supervised), the auxiliary segmentation ground truth is used during training. We show that the method provides fast and high-quality bone segmentation in the context of single-photon emission computed tomography (SPECT) image.},
  day      = {6},
  file     = {:Unsupervised Segmentation/Chen2020 - Medical Image Segmentation Via Unsupervised Convolutional Neural Network.pdf:PDF},
  groups   = {Unsupervised Segmentation},
  keywords = {image segmentation, active contour, convolutional neural network},
  month    = {7},
  year     = {2020},
}

@InProceedings{Attia2018,
  author   = {Attia, Alexandre and Dayan, Sharone},
  date     = {2018},
  title    = {Detection and segmentation of the Left Ventricle in Cardiac MRI using Deep Learning},
  abstract = {Manual segmentation of the Left Ventricle (LV) is a tedious and meticulous task that can vary depending on the patient, the Magnetic Resonance Images (MRI) cuts and the experts. Still today, we consider manual delineation done by experts as being the ground truth for cardiac diagnosticians. Thus, we are reviewing the paper-written by Avendi and al.-who presents a combined approach with Convolutional Neural Networks, Stacked Auto-Encoders and Deformable Models, to try and automate the segmentation while performing more accurately. Furthermore, we have implemented parts of the paper (around three quarts) and experimented both the original method and slightly modified versions when changing the architecture and the parameters.},
  file     = {:Unsupervised Segmentation/Attia2018 - Detection and Segmentation of the Left Ventricle in Cardiac MRI Using Deep Learning.pdf:PDF},
  groups   = {Unsupervised Segmentation},
}

@InProceedings{Sivanesan2019,
  author   = {Sivanesan, Umaseh and Braga, Luis and Sonnadara, Ranil and Dhindsa, Kiret},
  date     = {2019-11-14},
  title    = {Unsupervised Medical Image Segmentation with Adversarial Networks: From Edge Diagrams to Segmentation Maps},
  eprint   = {arXiv:1911.05140v1[eess.IV]},
  abstract = {We develop and approach to unsupervised semantic medical image segmentation that extends previous work with generative adversarial networks. We use existing edge detection methods to construct simple edge diagrams, train a generative model to convert them into synthetic medical images, and construct a dataset of synthetic images with known segmentations using variations on extracted edge diagrams. This synthetic dataset is then used to train a supervised image segmentation model. We test our approach on a clinical dataset of kidney ultrasound images and the benchmark ISIC 2018 skin lesion dataset. We show that our unsupervised approach is more accurate than previous unsupervised methods, and performs reasonably compared to supervised image segmentation models. All code and trained models are available at https://github.com/kiretd/Unsupervised-MIseg.},
  day      = {14},
  file     = {:Unsupervised Segmentation/Sivanesan2019 - Unsupervised Medical Image Segmentation with Adversarial Networks_ from Edge Diagrams to Segmentation Maps.pdf:PDF},
  groups   = {Unsupervised Segmentation},
  month    = {11},
  year     = {2019},
}

@InProceedings{Hamilton2022,
  author     = {Mark Hamilton and Zhoutong Zhang and Bharath Hariharan and Noah Snavely and W. Freeman},
  booktitle  = {International Conference on Learning Representations},
  date       = {2022},
  title      = {Unsupervised Semantic Segmentation by Distilling Feature Correspondences},
  doi        = {10.48550/arXiv.2203.08414},
  eprinttype = {arXiv},
  venue      = {ICLR},
  abstract   = {Unsupervised semantic segmentation aims to discover and localize semantically meaningful categories within image corpora without any form of annotation. To solve this task, algorithms must produce features for every pixel that are both semantically meaningful and compact enough to form distinct clusters. Unlike previous works which achieve this with a single end-to-end framework, we propose to separate feature learning from cluster compactification. Empirically, we show that current unsupervised feature learning frameworks already generate dense features whose correlations are semantically consistent. This observation motivates us to design STEGO (Self-supervised Transformer with Energy-based Graph Optimization), a novel framework that distills unsupervised features into highquality discrete semantic labels. At the core of STEGO is a novel contrastive loss function that encourages features to form compact clusters while preserving their relationships across the corpora. STEGO yields a significant improvement over the prior state of the art, on both the CocoStuff (+14 mIoU) and Cityscapes (+9 mIoU) semantic segmentation challenges.},
  file       = {:Unsupervised Segmentation, Transformer/Hamilton2022 - Unsupervised Semantic Segmentation by Distilling Feature Correspondences.pdf:PDF},
  groups     = {Unsupervised Segmentation, Transformer},
}

@WWW{mhamilton7232022,
  author = {mhamilton723},
  date   = {2022-06-21},
  title  = {STEGO on GitHub},
  url    = {https://github.com/mhamilton723/STEGO},
  groups = {Unsupervised Segmentation, Transformer},
}

@InProceedings{Caron2021a,
  author    = {Mathilde Caron and Hugo Touvron and Ishan Misra Hervé Jegou},
  booktitle = {2021 {IEEE}/{CVF} International Conference on Computer Vision ({ICCV})},
  date      = {2021},
  title     = {Supplementary Material for Emerging Properties in Self-Supervised Vision Transformers},
  file      = {:Unsupervised Segmentation, Transformer/Caron2021a - Supplementary Material for Emerging Properties in Self Supervised Vision Transformers.pdf:PDF},
  groups    = {Unsupervised Segmentation, Transformer},
}

@Article{Zhou2017,
  author       = {Zhou, Xiangrong and Takayama, Ryosuke and Wang, Song and Hara, Takeshi and Fujita, Hiroshi},
  date         = {2017},
  journaltitle = {Medical physics},
  title        = {Deep learning of the sectional appearances of 3D CT images for anatomical structure segmentation based on an FCN voting method},
  doi          = {10.1002/mp.12480]},
  volume       = {44},
  abstract     = {Purpose: We propose a single network trained by pixel-to-label deep learning to address the general issue of automatic multiple organ segmentation in three-dimensional (3D) computed tomography (CT) images. Our method can be described as a voxel-wise multiple-class classification scheme for automatically assigning labels to each pixel/voxel in a 2D/3D CT image. Methods: We simplify the segmentation algorithms of anatomical structures (including multiple organs) in a CT image (generally in 3D) to a majority voting scheme over the semantic segmentation of multiple 2D slices drawn from different viewpoints with redundancy. The proposed method inherits the spirit of fully convolutional networks (FCNs) that consist of "convolution" and "deconvolution" layers for 2D semantic image segmentation, and expands the core structure with 3D-2D-3D transformations to adapt to 3D CT image segmentation. All parameters in the proposed network are trained pixel-to-label from a small number of CT cases with human annotations as the ground truth. The proposed network naturally fulfills the requirements of multiple organ segmentations in CT cases of different sizes that cover arbitrary scan regions without any adjustment. Results: The proposed network was trained and validated using the simultaneous segmentation of 19 anatomical structures in the human torso, including 17 major organs and two special regions (lumen and content inside of stomach). Some of these structures have never been reported in previous research on CT segmentation. A database consisting of 240 (95\% for training and 5\% for testing) 3D CT scans, together with their manually annotated ground-truth segmentations, was used in our experiments. The results show that the 19 structures of interest were segmented with acceptable accuracy (88.1\% and 87.9\% voxels in the training and testing datasets, respectively, were labeled correctly) against the ground truth. Conclusions: We propose a single network based on pixel-to-label deep learning to address the challenging issue of anatomical structure segmentation in 3D CT cases. The novelty of this work is the policy of deep learning of the different 2D sectional appearances of 3D anatomical structures for CT cases and the majority voting of the 3D segmentation results from multiple crossed 2D sections to achieve availability and reliability with better efficiency, generality, and flexibility than conventional segmentation methods, which must be guided by human expertise.},
  booktitle    = {Medical physics},
  file         = {:3D-Segmentation/Zhou2017 - Deep Learning of the Sectional Appearances of 3D CT Images for Anatomical Structure Segmentation Based on an FCN Voting Method.pdf:PDF},
  groups       = {3D-Segmentation},
  keywords     = {2D semantic segmentation, anatomical structure segmentation, CT images, deep learning, fully convolutional network (FCN)},
  year         = {2017},
}

@TechReport{Steiner2022,
  author   = {Steiner, Andreas and Kolesnikov, Alexander and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  date     = {2022-05},
  title    = {How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
  number   = {researcher},
  abstract = {Vision Transformers (ViT) have been shown to attain highly competitive performance for a wide range of vision applications, such as image classification, object detection and semantic image segmentation. In comparison to convolutional neural networks, the Vision Transformer's weaker inductive bias is generally found to cause an increased reliance on model regularization or data augmentation ("AugReg" for short) when training on smaller training datasets. We conduct a systematic empirical study in order to better understand the interplay between the amount of training data, AugReg, model size and compute budget. 1 As one result of this study we find that the combination of increased compute and AugReg can yield models with the same performance as models trained on an order of magnitude more training data: we train ViT models of various sizes on the public ImageNet-21k dataset which either match or outperform their counterparts trained on the larger, but not publicly available JFT-300M dataset.},
  file     = {:Transformer/Steiner2022 - How to Train Your ViT_ Data, Augmentation, and Regularization in Vision Transformers.pdf:PDF},
  groups   = {Transformer},
}

@InProceedings{Chen2021,
  author    = {Xinlei Chen and Saining Xie and Kaiming He},
  booktitle = {2021 {IEEE/CVF} International Conference on Computer Vision ({ICCV})},
  date      = {2021},
  title     = {An Empirical Study of Training Self-Supervised Vision Transformers},
  doi       = {10.1109/iccv48922.2021.00950},
  abstract  = {This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: selfsupervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other selfsupervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.},
  file      = {:Transformer/Chen2021 - An Empirical Study of Training Self Supervised Vision Transformers.pdf:PDF},
  groups    = {Transformer},
}

@InProceedings{Dosovitskiy2021,
  author    = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  booktitle = {Proceedings of the International Conference on Learning Representations 2021},
  date      = {2021},
  title     = {AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE},
  abstract  = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train. 1},
  file      = {:Transformer/Dosovitskiy2021 - AN IMAGE IS WORTH 16X16 WORDS_ TRANSFORMERS fOR IMAGE RECOGNITION aT SCALE.pdf:PDF},
  groups    = {Transformer},
}

@InProceedings{Vaswani2017,
  author    = {Vaswani, Ashish and Brain, Google and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan and Kaiser, Łukasz},
  booktitle = {Advances in Neural Information Processing Systems},
  date      = {2017},
  title     = {Attention Is All You Need},
  abstract  = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
  file      = {:Transformer/Vaswani2017 - Attention Is All You Need.pdf:PDF},
  groups    = {Transformer},
}

@InProceedings{Goodfellow2020,
  author   = {Ian Goodfellow and Jean Pouget-Abadie and Mehdi Mirza and Bing Xu and David Warde-Farley and Sherjil Ozair and Aaron Courville and Yoshua Bengio},
  date     = {2020},
  title    = {Generative adversarial networks},
  doi      = {10.1145/3422622},
  pages    = {139-144},
  volume   = {63},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1 2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  file     = {:Neural Networks/Goodfellow2020 - Generative Adversarial Networks.pdf:PDF},
  groups   = {Neural Networks},
  issn     = {0001-0782},
}

@InProceedings{Kalyan2021,
  author     = {Katikapalli Subramanyam Kalyan and A. Rajasekharan and S. Sangeetha},
  date       = {2021},
  title      = {AMMUS: A Survey of Transformer-based Pretrained Models in Natural Language Processing},
  eprint     = {2108.05542},
  eprinttype = {arXiv},
  url        = {https://www.semanticscholar.org/paper/6c761cfdb031701072582e434d8f64d436255da6},
  venue      = {ArXiv},
  abstract   = {Transformer-based pretrained language models (T-PTLMs) have achieved great success in almost every NLP task. The evolution of these models started with GPT and BERT. These models are built on the top of transformers, self-supervised learning and transfer learning. Transformed-based PTLMs learn universal language representations from large volumes of text data using self-supervised learning and transfer this knowledge to downstream tasks. These models provide good background knowledge to downstream tasks which avoids training of downstream models from scratch. In this comprehensive survey paper, we initially give a brief overview of self-supervised learning. Next, we explain various core concepts like pretraining, pretraining methods, pretraining tasks, embeddings and downstream adaptation methods. Next, we present a new taxonomy of T-PTLMs and then give brief overview of various benchmarks including both intrinsic and extrinsic. We present a summary of various useful libraries to work with T-PTLMs. Finally, we highlight some of the future research directions which will further improve these models. We strongly believe that this comprehensive survey paper will serve as a good reference to learn the core concepts as well as to stay updated with the recent happenings in T-PTLMs. The list of T-PTLMs along with links is available at https://mr-nlp.github.io/posts/2021/05/tptlms-list/},
  file       = {:Transformer/Kalyan2021 - AMMUS_ a Survey of Transformer Based Pretrained Models in Natural Language Processing.pdf:PDF},
  groups     = {Transformer},
  keywords   = {Self-Supervised Learning, Transformers, Pretrained Language Models, Survey},
}

@Article{Lin2021,
  author       = {Tianyang Lin and Yuxin Wang and Xiangyang Liu and Xipeng Qiu},
  date         = {2021},
  journaltitle = {AI Open},
  title        = {A Survey of Transformers},
  doi          = {10.1016/j.aiopen.2022.10.001},
  volume       = {3},
  abstract     = {Transformers have achieved great success in many artificial intelligence fields, such as natural language processing, computer vision, and audio processing. Therefore, it is natural to attract lots of interest from academic and industry researchers. Up to the present, a great variety of Transformer variants (a.k.a. X-formers) have been proposed, however, a systematic and comprehensive literature review on these Transformer variants is still missing. In this survey, we provide a comprehensive review of various X-formers. We first briefly introduce the vanilla Transformer and then propose a new taxonomy of X-formers. Next, we introduce the various X-formers from three perspectives: architectural modification, pre-training, and applications. Finally, we outline some potential directions for future research.},
  file         = {:Transformer/1-s2.0-S2666651022000146-main.pdf:PDF},
  groups       = {Transformer},
  keywords     = {Transformer, Self-Attention, Pre-trained Models, Deep Learning},
  year         = {2021},
}

@InProceedings{Liu2019,
  author    = {Liu, Yang and Zhang, Yao and Wang, Yixin and Hou, Feng and Yuan, Jin and Tian, Jiang and Zhang, Yang and Shi, Zhongchao and Fan, Jianping and He, Zhiqiang and Zhang, Yao and Zhang, Yang},
  booktitle = {arXiv preprint},
  date      = {2019},
  title     = {A Survey of Visual Transformers},
  abstract  = {Transformer, an attention-based encoder-decoder model, has already revolutionized the field of natural language processing (NLP). Inspired by such significant achievements, some pioneering works have recently been done on employing Transformer-liked architectures in the computer vision (CV) field, which have demonstrated their effectiveness on three fundamental CV tasks (classification, detection, and segmentation) as well as multiple sensory data stream (images, point clouds, and vision-language data). Because of their competitive modeling capabilities, the visual Transformers have achieved impressive performance improvements over multiple benchmarks as compared with modern Convolution Neural Networks (CNNs). In this survey, we have reviewed over one hundred of different visual Transformers comprehensively according to three fundamental CV tasks and different data stream types, where a taxonomy is proposed to organize the representative methods according to their motivations, structures, and application scenarios. Because of their differences on training settings and dedicated vision tasks, we have also evaluated and compared all these existing visual Transformers under different configurations. Furthermore, we have revealed a series of essential but unexploited aspects that may empower such visual Transformers to stand out from numerous architectures, e.g., slack high-level semantic embeddings to bridge the gap between the visual Transformers and the sequential ones. Finally, three promising research directions are suggested for future investment. We will continue to update the latest articles and their released source codes at https://github.com/liuyang-ict/awesome-visual-transformers.},
  file      = {:Transformer/Liu2019 - A Survey of Visual Transformers.pdf:PDF},
  groups    = {Transformer},
  keywords  = {Visual Transformer, attention, high-level vision, 3D point clouds, multi-sensory data stream, multi-modal, visuallinguistic pre-training, self-supervision, neural networks, computer vision},
}

@TechReport{He2022,
  author    = {He, Kelei and Gan, Chen and Li, Zhuoyuan and Rekik, Islem and Yin, Zihao and Ji, Wen and Gao, Yang and Wang, Qian and Zhang, Junfeng and Shen, Dinggang and He, R and Gao and Shen},
  date      = {2022},
  title     = {Transformers in Medical Image Analysis: A Review},
  number    = {chrono-},
  abstract  = {Transformers have dominated the field of natural language processing, and recently impacted the computer vision area. In the field of medical image analysis, Transformers have also been successfully applied to full-stack clinical applications, including image synthesis/reconstruction, registration, segmentation, detection, and diagnosis. Our paper aims to promote awareness and application of Transformers in the field of medical image analysis. Specifically, we first overview the core concepts of the attention mechanism built into Transformers and other basic components. Second, we review various Transformer architectures tailored for medical image applications and discuss their limitations. Within this review, we investigate key challenges revolving around the use of Transformers in different learning paradigms, improving the model efficiency, and their coupling with other techniques. We hope this review can give a comprehensive picture of Transformers to the readers in the field of medical image analysis.},
  file      = {:Transformer/He2022 - Transformers in Medical Image Analysis_ a Review.pdf:PDF},
  groups    = {Transformer},
  keywords  = {Index Terms-Transformers, Medical image analysis, Deep learning, Diagnosis, Registration, Segmentation, Image synthesis, Multi-task learning, Multi-modal learning, Weakly-supervised learning},
  publisher = {IEEE},
}

@InProceedings{Devlin2019,
  author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle = {Proceedings of Annual Conference of the North American Association for Computational Linguistics Meeting 2019},
  date      = {2019},
  title     = {BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspecific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  file      = {:Transformer/Devlin2019 - BERT_ Pre Training of Deep Bidirectional Transformers for Language Understanding.pdf:PDF},
  groups    = {Transformer},
}

@InProceedings{Geva2021,
  author    = {Geva, Mor and Schuster, Roei and Berant, Jonathan and Levy, Omer},
  booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing 2021},
  date      = {2021},
  title     = {Transformer Feed-Forward Layers Are Key-Value Memories},
  doi       = {10.18653/v1/2021.emnlp-main.446},
  abstract  = {Feed-forward layers constitute two-thirds of a transformer model's parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformerbased language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary. Our experiments show that the learned patterns are human-interpretable, and that lower layers tend to capture shallow patterns, while upper layers learn more semantic ones. The values complement the keys' input patterns by inducing output distributions that concentrate probability mass on tokens likely to appear immediately after each pattern, particularly in the upper layers. Finally, we demonstrate that the output of a feed-forward layer is a composition of its memories, which is subsequently refined throughout the model's layers via residual connections to produce the final output distribution.},
  file      = {:Transformer/Geva2021 - Transformer Feed Forward Layers Are Key Value Memories.pdf:PDF},
  groups    = {Transformer},
}

@InProceedings{Press2017,
  author    = {Ofir Press and Lior Wolf},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics},
  date      = {2017},
  title     = {Using the Output Embedding to Improve Language Models},
  doi       = {10.18653/v1/e17-2025},
  abstract  = {We study the topmost weight matrix of neural network language models. We show that this matrix constitutes a valid word embedding. When training language models, we recommend tying the input embedding and this output embedding. We analyze the resulting update rules and show that the tied embedding evolves in a more similar way to the output embedding than to the input embedding in the untied model. We also offer a new method of regularizing the output embedding. Our methods lead to a significant reduction in perplexity, as we are able to show on a variety of neural network language models. Finally, we show that weight tying can reduce the size of neural translation models to less than half of their original size without harming their performance.},
  file      = {:Transformer/Press2017 - Using the Output Embedding to Improve Language Models.pdf:PDF},
  groups    = {Transformer},
}

@InProceedings{Wang2019,
  author    = {Qiang Wang and Bei Li and Tong Xiao and Jingbo Zhu and Changliang Li and Derek F. Wong and Lidia S. Chao},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  date      = {2019},
  title     = {Learning Deep Transformer Models for Machine Translation},
  doi       = {10.18653/v1/p19-1176},
  publisher = {Association for Computational Linguistics},
  abstract  = {Transformer is the state-of-the-art model in recent machine translation evaluations. Two strands of research are promising to improve models of this kind: the first uses wide networks (a.k.a. Transformer-Big) and has been the de facto standard for the development of the Transformer system, and the other uses deeper language representation but faces the difficulty arising from learning deep networks. Here, we continue the line of research on the latter. We claim that a truly deep Transformer model can surpass the Transformer-Big counterpart by 1) proper use of layer normalization and 2) a novel way of passing the combination of previous layers to the next. On WMT'16 English-German, NIST OpenMT'12 Chinese-English and larger WMT'18 Chinese-English tasks, our deep system (30/25-layer encoder) outperforms the shallow Transformer-Big/Base baseline (6-layer encoder) by 0.4∼2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big 1 .},
  file      = {:Transformer/Wang2019 - Learning Deep Transformer Models for Machine Translation.pdf:PDF},
  groups    = {Transformer},
}

@InProceedings{Kraehenb2012,
  author   = {Krähenb, Philipp and Koltun, Vladlen},
  date     = {2012-10-20},
  title    = {Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials},
  eprint   = {arXiv:1210.5644v1[cs.CV]},
  abstract = {Most state-of-the-art techniques for multi-class image segmentation and labeling use conditional random fields defined over pixels or image regions. While regionlevel models often feature dense pairwise connectivity, pixel-level models are considerably larger and have only permitted sparse graph structures. In this paper, we consider fully connected CRF models defined on the complete set of pixels in an image. The resulting graphs have billions of edges, making traditional inference algorithms impractical. Our main contribution is a highly efficient approximate inference algorithm for fully connected CRF models in which the pairwise edge potentials are defined by a linear combination of Gaussian kernels. Our experiments demonstrate that dense connectivity at the pixel level substantially improves segmentation and labeling accuracy.},
  day      = {20},
  file     = {:Unsupervised Segmentation/Kraehenb2012 - Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials.pdf:PDF},
  groups   = {Unsupervised Segmentation},
  keywords = {DenseCRF, Fully Connected, CRF, Connected, Efficient inference.},
  month    = {10},
  year     = {2012},
}

@InProceedings{Wang2021,
  author    = {Feng Wang and Huaping Liu},
  booktitle = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  date      = {2021},
  title     = {Understanding the Behaviour of Contrastive Loss},
  doi       = {10.1109/cvpr46437.2021.00252},
  publisher = {{IEEE}},
  abstract  = {Unsupervised contrastive learning has achieved outstanding success, while the mechanism of contrastive loss has been less studied. In this paper, we concentrate on the understanding of the behaviours of unsupervised contrastive loss. We will show that the contrastive loss is a hardness-aware loss function, and the temperature τ controls the strength of penalties on hard negative samples. The previous study has shown that uniformity is a key property of contrastive learning. We build relations between the uniformity and the temperature τ. We will show that uniformity helps the contrastive learning to learn separable features, however excessive pursuit to the uniformity makes the contrastive loss not tolerant to semantically similar samples, which may break the underlying semantic structure and be harmful to the formation of features useful for downstream tasks. This is caused by the inherent defect of the instance discrimination objective. Specifically, instance discrimination objective tries to push all different instances apart, ignoring the underlying relations between samples. Pushing semantically consistent samples apart has no positive effect for acquiring a prior informative to general downstream tasks. A well-designed contrastive loss should have some extents of tolerance to the closeness of semantically similar samples. Therefore, we find that the contrastive loss meets a uniformity-tolerance dilemma, and a good choice of temperature can compromise these two properties properly to both learn separable features and tolerant to semantically similar samples, improving the feature qualities and the downstream performances.},
  file      = {:Unsupervised Segmentation/Wang2021 - Understanding the Behaviour of Contrastive Loss.pdf:PDF},
  groups    = {Unsupervised Segmentation},
}

@Article{Picard2021,
  author     = {David Picard},
  date       = {2021},
  title      = {Torch.manual_seed(3407) is all you need: On the influence of random seeds in deep learning architectures for computer vision},
  eprint     = {2109.08203},
  eprinttype = {arXiv},
  url        = {https://www.semanticscholar.org/paper/29f54c56428f015e485184cbd4820f7561d9b967},
  abstract   = {In this paper I investigate the effect of random seed selection on the accuracy when using popular deep learning architectures for computer vision. I scan a large amount of seeds (up to 10) on CIFAR 10 and I also scan fewer seeds on Imagenet using pre-trained models to investigate large scale datasets. The conclusions are that even if the variance is not very large, it is surprisingly easy to find an outlier that performs much better or much worse than the average.},
  file       = {:Misc/Picard2021 - Torch.manual_seed(3407) Is All You Need_ on the Influence of Random Seeds in Deep Learning Architectures for Computer Vision.pdf:PDF},
  groups     = {Misc},
  keywords   = {Deep Learning, Computer Vision, Randomness},
  venue      = {ArXiv},
}

@InProceedings{Cho2021,
  author       = {Jang Hyun Cho and Utkarsh Mall and Kavita Bala and Bharath Hariharan},
  booktitle    = {2021 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
  date         = {2021},
  title        = {{PiCIE}: Unsupervised Semantic Segmentation using Invariance and Equivariance in Clustering},
  doi          = {10.1109/cvpr46437.2021.01652},
  publisher    = {{IEEE}},
  file         = {:Unsupervised Segmentation/Cho2021 - PiCIE_ Unsupervised Semantic Segmentation Using Invariance and Equivariance in Clustering.pdf:PDF},
  groups       = {Unsupervised Segmentation},
  journaltitle = {IEEE Conference on Computer Vision and Pattern Recognition},
}

@WWW{Coccomini2021,
  author  = {Davide Coccomini},
  date    = {2021-07-23},
  title   = {Vision Transformers or Convolutional Neural Networks? Both!},
  url     = {https://towardsdatascience.com/vision-transformers-or-convolutional-neural-networks-both-de1a2c3c62e4},
  urldate = {2022-12-05},
  groups  = {Transformer},
}

@InProceedings{Ji2019,
  author    = {Xu Ji and Andrea Vedaldi and Joao Henriques},
  booktitle = {2019 {IEEE/CVF} International Conference on Computer Vision (ICCV)},
  date      = {2019},
  title     = {Invariant Information Clustering for Unsupervised Image Classification and Segmentation},
  doi       = {10.1109/iccv.2019.00996},
  abstract  = {We present a novel clustering objective that learns a neural network classifier from scratch, given only unlabelled data samples. The model discovers clusters that accurately match semantic classes, achieving state-of-the-art results in eight unsupervised clustering benchmarks spanning image classification and segmentation. These include STL10, an unsupervised variant of ImageNet, and CIFAR10, where we significantly beat the accuracy of our closest competitors by 6.6 and 9.5 absolute percentage points respectively. The method is not specialised to computer vision and operates on any paired dataset samples; in our experiments we use random transforms to obtain a pair from each image. The trained network directly outputs semantic labels, rather than high dimensional representations that need external processing to be usable for semantic clustering. The objective is simply to maximise mutual information between the class assignments of each pair. It is easy to implement and rigorously grounded in information theory, meaning we effortlessly avoid degenerate solutions that other clustering methods are susceptible to. In addition to the fully unsupervised mode, we also test two semi-supervised settings. The first achieves 88.8 \% accuracy on STL10 classification, setting a new global state-of-the-art over all existing methods (whether supervised, semi-supervised or unsupervised). The second shows robustness to 90 \% reductions in label coverage, of relevance to applications that wish to make use of small amounts of labels.},
  file      = {:Unsupervised Segmentation/Ji2019 - Invariant Information Clustering for Unsupervised Image Classification and Segmentation.pdf:PDF},
  groups    = {Unsupervised Segmentation},
}

@Article{Harvey2002,
  author       = {Mark S. Harvey},
  date         = {2002},
  journaltitle = {The Journal of Arachnology},
  title        = {THE NEGLECTED COUSINS: WHAT DO WE KNOW ABOUT THE SMALLER ARACHNID ORDERS?},
  doi          = {10.1636/0161-8202(2002)030[0357:tncwdw]2.0.co;2},
  issn         = {0161-8202},
  pages        = {357-372},
  volume       = {30},
  abstract     = {BioOne Complete (complete.BioOne.org) is a full-text database of 200 subscribed and open-access titles in the biological, ecological, and environmental sciences published by nonprofit societies, associations, museums, institutions, and presses.},
  file         = {:Misc/Harvey2002 - THE NEGLECTED COUSINS_ WHAT DO WE KNOW aBOUT tHE SMALLER ARACHNID ORDERS_.pdf:PDF},
  groups       = {Misc},
  keywords     = {Arachnida, Opilioacariformes, Ricinulei, Palpigradi, Uropygi, Amblypygi, Schizomida, Solifugae, Pseudoscorpiones, diversity, systematics},
}

@WWW{rgommers2012,
  author  = {rgommers},
  date    = {2012-09-08},
  title   = {MAINT: silence Cython warnings about changes dtype/ufunc size.},
  url     = {https://github.com/numpy/numpy/pull/432},
  urldate = {2023-01-31},
  groups  = {Misc},
}

@Article{Webb2021,
  author       = {Jeremy M. Webb and Shaheeda A. Adusei and Yinong Wang and Naziya Samreen and Kalie Adler and Duane D. Meixner and Robert T. Fazzio and Mostafa Fatemi and Azra Alizad},
  date         = {2021},
  journaltitle = {Computer in Biology and Medicine},
  title        = {Comparing deep learning-based automatic segmentation of breast masses to expert interobserver variability in ultrasound imaging},
  doi          = {10.1016/j.compbiomed.2021.104966},
  issn         = {0010-4825},
  volume       = {139},
  file         = {:Segmentation/Webb2021 - Comparing Deep Learning Based Automatic Segmentation of Breast Masses to Expert Interobserver Variability in Ultrasound Imaging.pdf:PDF},
  groups       = {Segmentation},
}

@Book{James2013,
  author    = {James, Gareth},
  date      = {2013},
  title     = {An Introduction to Statistical Learning with Applications in R},
  doi       = {10.1007/978-1-4614-7138-7},
  isbn      = {9781461471387},
  publisher = {Springer Texts in Statistics},
  subtitle  = {with Applications in R},
  file      = {:ML&AI/James2013 - An Introduction to Statistical Learning with Applications in R.pdf:PDF},
  groups    = {ML&AI},
}

@Article{Bengio2004,
  author       = {Bengio, Yoshua and Grandvalet, Yves},
  date         = {2004},
  journaltitle = {Journal of Machine Learning Research},
  title        = {No Unbiased Estimator of the Variance of K-Fold Cross-Validation},
  abstract     = {Most machine learning researchers perform quantitative experiments to estimate generalization error and compare the performance of different algorithms (in particular, their proposed algorithm). In order to be able to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the very commonly used K-fold cross-validation estimator of generalization performance. The main theorem shows that there exists no universal (valid under all distributions) unbiased estimator of the variance of K-fold cross-validation. The analysis that accompanies this result is based on the eigen-decomposition of the covariance matrix of errors, which has only three different eigenvalues corresponding to three degrees of freedom of the matrix and three components of the total variance. This analysis helps to better understand the nature of the problem and how it can make naive estimators (that don't take into account the error correlations due to the overlap between training and test sets) grossly underestimate variance. This is confirmed by numerical experiments in which the three components of the variance are compared when the difficulty of the learning problem and the number of folds are varied.},
  file         = {:ML&AI/Bengio2004 - No Unbiased Estimator of the Variance of K Fold Cross Validation.pdf:PDF},
  groups       = {ML&AI},
  keywords     = {cross-validation, variance estimators, k-fold cross-validation, statistical comparisons of algorithms},
}

@InCollection{Ganguly2010,
  author    = {Debashis Ganguly and Srabonti Chakraborty and Maricel Balitanas and Tai-hoon Kim},
  booktitle = {Communications in Computer and Information Science},
  date      = {2010},
  title     = {Medical Imaging: A Review},
  doi       = {10.1007/978-3-642-16444-6_63},
  publisher = {Springer Berlin Heidelberg},
  abstract  = {The rapid progress of medical science and the invention of various medicines have benefited mankind and the whole civilization. Modern science also has been doing wonders in the surgical field. But, the proper and correct diagnosis of diseases is the primary necessity before the treatment. The more sophisticate the bio-instruments are, better diagnosis will be possible. The medical images plays an important role in clinical diagnosis and therapy of doctor and teaching and researching etc. Medical imaging is often thought of as a way to represent anatomical structures of the body with the help of X-ray computed tomography and magnetic resonance imaging. But often it is more useful for physiologic function rather than anatomy. With the growth of computer and image technology medical imaging has greatly influenced medical field. As the quality of medical imaging affects diagnosis the medical image processing has become a hotspot and the clinical applications wanting to store and retrieve images for future purpose needs some convenient process to store those images in details. This paper is a tutorial review of the medical image processing and repository techniques appeared in the literature.},
  file      = {:Segmentation/Ganguly2010 - Medical Imaging_ a Review.pdf:PDF},
  groups    = {Segmentation},
  keywords  = {bio-instruments, tomography, magnetic resonance, physiologic, anatomy, clinical},
}

@Article{Cheplygina2019,
  author       = {Veronika Cheplygina},
  date         = {2019},
  journaltitle = {Current Opinion in Biomedical Engineering},
  title        = {Cats or {CAT} scans: Transfer learning from natural or medical image source data sets?},
  doi          = {10.1016/j.cobme.2018.12.005},
  volume       = {9},
  abstract     = {Transfer learning is a widely used strategy in medical image analysis. Instead of only training a network with a limited amount of data from the target task of interest, we can first train the network with other, potentially larger source datasets, creating a more robust model. The source datasets do not have to be related to the target task. For a classification task in lung CT images, we could use both head CT images, or images of cats, as the source. While head CT images appear more similar to lung CT images, the number and diversity of cat images might lead to a better model overall. In this survey we review a number of papers that have performed similar comparisons. Although the answer to which strategy is best seems to be "it depends", we discuss a number of research directions we need to take as a community, to gain more understanding of this topic.},
  file         = {:Segmentation/Cheplygina2019 - Cats or CAT Scans_ Transfer Learning from Natural or Medical Image Source Data Sets_.pdf:PDF},
  groups       = {Segmentation},
  keywords     = {medical imaging, deep learning, transfer learning},
  publisher    = {Elsevier {BV}},
}

@Article{Han2023,
  author       = {Kai Han and Yunhe Wang and Hanting Chen and Xinghao Chen and Jianyuan Guo and Zhenhua Liu and Yehui Tang and An Xiao and Chunjing Xu and Yixing Xu and Zhaohui Yang and Yiman Zhang and Dacheng Tao},
  date         = {2023-01},
  journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
  title        = {A Survey on Vision Transformer},
  doi          = {10.1109/tpami.2022.3152247},
  number       = {1},
  pages        = {87--110},
  volume       = {45},
  abstract     = {Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the selfattention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.},
  file         = {:Transformer/Han2023 - A Survey on Vision Transformer.pdf:PDF},
  groups       = {Transformer},
  keywords     = {Computer vision, high-level vision, low-level vision, self-attention, transformer, video},
  publisher    = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@Comment{jabref-meta: databaseType:biblatex;}

@Comment{jabref-meta: fileDirectoryLatex-ahrensj-zitpcx36556:/home/ahrensj/Nextcloud/Jennys/BSc;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Segmentation\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Unsupervised Segmentation\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:Semi-Supervised Segmentation\;0\;1\;0x8a8a8aff\;\;\;;
2 StaticGroup:3D-Segmentation\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:ML&AI\;0\;1\;0x8a8a8aff\;\;Machine Learning and AI overview books\;;
2 StaticGroup:DL\;0\;1\;0x8a8a8aff\;\;\;;
3 StaticGroup:Neural Networks\;0\;1\;0x8a8a8aff\;\;\;;
4 StaticGroup:CNN\;0\;0\;0x8a8a8aff\;\;\;;
4 StaticGroup:Transformer\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Software\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:DESY\;0\;1\;0x8a8a8aff\;\;\;;
1 StaticGroup:Misc\;0\;1\;0x8a8a8aff\;\;\;;
}

@Comment{jabref-meta: saveActions:disabled;
all-text-fields[identity]
date[normalize_date]
month[normalize_month]
pages[normalize_page_numbers]
;}
